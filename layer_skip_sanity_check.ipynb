{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Skip Sanity Check\n",
    "\n",
    "éªŒè¯ `generate_with_layer_skip` åœ¨ `layer_skip_threshold=1.0` æ—¶é€€åŒ–æˆæ™®é€š `generate_with_dual_cache` ç‰ˆæœ¬ã€‚\n",
    "\n",
    "**éªŒè¯é€»è¾‘ï¼š**\n",
    "- å½“ threshold=1.0 æ—¶ï¼Œcos_sim ä¸å¯èƒ½ > 1ï¼Œæ‰€ä»¥ä¸ä¼šè·³è¿‡ä»»ä½•å±‚\n",
    "- å› æ­¤ç»“æœåº”è¯¥ä¸æ™®é€š dual_cache å®Œå…¨ä¸€è‡´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.chdir('llada')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pianng/miniconda3/envs/dllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: GSAI-ML/LLaDA-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model.modeling_llada import LLaDAModelLM\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"âœ… Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Functions imported!\n"
     ]
    }
   ],
   "source": [
    "from generate import (\n",
    "    generate_with_dual_cache,\n",
    "    generate_with_layer_skip,\n",
    ")\n",
    "\n",
    "print(\"âœ… Functions imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sanity Check: threshold=1.0 åº”è¯¥ä¸ dual_cache å®Œå…¨ä¸€è‡´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(prompt_text, tokenizer, device):\n",
    "    m = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    input_ids = tokenizer(prompt)['input_ids']\n",
    "    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Sanity Check: layer_skip_threshold=1.0 vs dual_cache\n",
      "======================================================================\n",
      "\n",
      "[Test 1/4] What is 2 + 2?...\n",
      "   âœ… PASSED\n",
      "   Tokens match: True\n",
      "   NFE: orig=5, skip=5\n",
      "   Skip stats: computed=160, skipped=0\n",
      "\n",
      "[Test 2/4] Who is Newton?...\n",
      "   âœ… PASSED\n",
      "   Tokens match: True\n",
      "   NFE: orig=17, skip=17\n",
      "   Skip stats: computed=544, skipped=0\n",
      "\n",
      "[Test 3/4] Lily can run 12 km/h for 4 hours. After ...\n",
      "   âœ… PASSED\n",
      "   Tokens match: True\n",
      "   NFE: orig=32, skip=32\n",
      "   Skip stats: computed=1024, skipped=0\n",
      "\n",
      "[Test 4/4] Explain the concept of machine learning ...\n",
      "   âœ… PASSED\n",
      "   Tokens match: True\n",
      "   NFE: orig=23, skip=23\n",
      "   Skip stats: computed=736, skipped=0\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ Sanity Check PASSED! threshold=1.0 correctly degrades to dual_cache.\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ç”¨çš„ prompts\n",
    "test_prompts = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Who is Newton?\",\n",
    "    \"Lily can run 12 km/h for 4 hours. After that, she runs 6 km/h. How many km can she run in 8 hours?\",\n",
    "    \"Explain the concept of machine learning in one sentence.\",\n",
    "]\n",
    "\n",
    "# ç”Ÿæˆå‚æ•°\n",
    "gen_params = {\n",
    "    'steps': 32,\n",
    "    'gen_length': 64,\n",
    "    'block_length': 32,\n",
    "    'temperature': 0.,\n",
    "    'remasking': 'low_confidence',\n",
    "    'threshold': 0.9,\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Sanity Check: layer_skip_threshold=1.0 vs dual_cache\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_passed = True\n",
    "\n",
    "for i, prompt_text in enumerate(test_prompts):\n",
    "    print(f\"\\n[Test {i+1}/{len(test_prompts)}] {prompt_text[:40]}...\")\n",
    "    \n",
    "    input_ids = prepare_input(prompt_text, tokenizer, device)\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    \n",
    "    # 1. åŸç‰ˆ dual_cache\n",
    "    x_orig, nfe_orig = generate_with_dual_cache(model, input_ids, **gen_params)\n",
    "    answer_orig = tokenizer.decode(x_orig[0, prompt_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # 2. layer_skip with threshold=1.0 (should be identical)\n",
    "    x_skip, nfe_skip, stats = generate_with_layer_skip(\n",
    "        model, input_ids, layer_skip_threshold=1.0, **gen_params\n",
    "    )\n",
    "    answer_skip = tokenizer.decode(x_skip[0, prompt_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # éªŒè¯\n",
    "    tokens_match = torch.equal(x_orig, x_skip)\n",
    "    nfe_match = nfe_orig == nfe_skip\n",
    "    \n",
    "    passed = tokens_match and nfe_match\n",
    "    all_passed = all_passed and passed\n",
    "    \n",
    "    status = \"âœ… PASSED\" if passed else \"âŒ FAILED\"\n",
    "    print(f\"   {status}\")\n",
    "    print(f\"   Tokens match: {tokens_match}\")\n",
    "    print(f\"   NFE: orig={nfe_orig}, skip={nfe_skip}\")\n",
    "    print(f\"   Skip stats: computed={stats['total_layers_computed']}, skipped={stats['total_layers_skipped']}\")\n",
    "    \n",
    "    if not tokens_match:\n",
    "        print(f\"   âš ï¸  Answer orig: {answer_orig[:50]}...\")\n",
    "        print(f\"   âš ï¸  Answer skip: {answer_skip[:50]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_passed:\n",
    "    print(\"ğŸ‰ Sanity Check PASSED! threshold=1.0 correctly degrades to dual_cache.\")\n",
    "else:\n",
    "    print(\"âŒ Sanity Check FAILED! There's a bug in the implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æµ‹è¯•ä¸åŒ threshold ä¸‹çš„è·³è¿‡æƒ…å†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Testing different layer_skip_thresholds\n",
      "======================================================================\n",
      "\n",
      "Baseline (dual_cache): NFE=32\n",
      "Answer: In the first 4 hours, Lily runs 12 km/h 4 = 48 km.\n",
      " the 6 km/h 4 = 48 km + 24 km = 72 km.\n",
      ": 72\n",
      "\n",
      "Threshold=1.000:\n",
      "   Computed: 1024, Skipped: 0\n",
      "   Skip rate: 0.0%\n",
      "   Match baseline: âœ…\n",
      "\n",
      "Threshold=0.993:\n",
      "   Computed: 993, Skipped: 31\n",
      "   Skip rate: 3.0%\n",
      "   Match baseline: âœ…\n",
      "\n",
      "Threshold=0.989:\n",
      "   Computed: 938, Skipped: 86\n",
      "   Skip rate: 8.4%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In the first 4 hours, runs 12 km/h 4 = 48 km.\n",
      "In the 4 hours...\n",
      "\n",
      "Threshold=0.985:\n",
      "   Computed: 899, Skipped: 125\n",
      "   Skip rate: 12.2%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In the first 4 hours, she runs 12 km/h 4 = 48 km.\n",
      " the she r...\n",
      "\n",
      "Threshold=0.981:\n",
      "   Computed: 851, Skipped: 173\n",
      "   Skip rate: 16.9%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In the first 4 hours, Lily runs 12 km/h 4 = 48 km.\n",
      " the she ...\n",
      "\n",
      "Threshold=0.978:\n",
      "   Computed: 682, Skipped: 150\n",
      "   Skip rate: 18.0%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In the first 4 hours, 4 hours x 12 km/h = 48 km.\n",
      "In the 4 ho...\n",
      "\n",
      "Threshold=0.974:\n",
      "   Computed: 828, Skipped: 196\n",
      "   Skip rate: 19.1%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In the first 4 hours, Lily can run 12 km/h 4 = 48 km., she c...\n",
      "\n",
      "Threshold=0.970:\n",
      "   Computed: 817, Skipped: 207\n",
      "   Skip rate: 20.2%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In first run, she runs 12 km/h * 4 hours = 48 km * 4 hours =...\n",
      "\n",
      "Threshold=0.966:\n",
      "   Computed: 440, Skipped: 104\n",
      "   Skip rate: 19.1%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In 4 hours, she runs 12 km  km...\n",
      "\n",
      "Threshold=0.962:\n",
      "   Computed: 431, Skipped: 113\n",
      "   Skip rate: 20.8%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In 2 hours, she can run 12 km ...\n",
      "\n",
      "Threshold=0.958:\n",
      "   Computed: 354, Skipped: 94\n",
      "   Skip rate: 21.0%\n",
      "   Match baseline: âŒ\n",
      "   Answer: In 4 hours, \\ runs 48 km...\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ä¸åŒ threshold\n",
    "test_prompt = \"Lily can run 12 km/h for 4 hours. After that, she runs 6 km/h. How many km can she run in 8 hours?\"\n",
    "input_ids = prepare_input(test_prompt, tokenizer, device)\n",
    "prompt_len = input_ids.shape[1]\n",
    "\n",
    "# æ ¹æ®ç›®æ ‡ skip_rate è®¾å®šçš„ threshold\n",
    "thresholds = [1.000, 0.993, 0.989, 0.985, 0.981, 0.978, 0.974, 0.970, 0.966, 0.962, 0.958]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Testing different layer_skip_thresholds\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Baseline\n",
    "x_baseline, nfe_baseline = generate_with_dual_cache(model, input_ids, **gen_params)\n",
    "answer_baseline = tokenizer.decode(x_baseline[0, prompt_len:], skip_special_tokens=True)\n",
    "print(f\"\\nBaseline (dual_cache): NFE={nfe_baseline}\")\n",
    "print(f\"Answer: {answer_baseline}\")\n",
    "\n",
    "results = []\n",
    "for thresh in thresholds:\n",
    "    x, nfe, stats = generate_with_layer_skip(\n",
    "        model, input_ids, layer_skip_threshold=thresh, **gen_params\n",
    "    )\n",
    "    answer = tokenizer.decode(x[0, prompt_len:], skip_special_tokens=True)\n",
    "    match = torch.equal(x, x_baseline)\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'nfe': nfe,\n",
    "        'computed': stats['total_layers_computed'],\n",
    "        'skipped': stats['total_layers_skipped'],\n",
    "        'skip_rate': stats['skip_rate'],\n",
    "        'match_baseline': match,\n",
    "        'answer': answer,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nThreshold={thresh:.3f}:\")\n",
    "    print(f\"   Computed: {stats['total_layers_computed']}, Skipped: {stats['total_layers_skipped']}\")\n",
    "    print(f\"   Skip rate: {stats['skip_rate']*100:.1f}%\")\n",
    "    print(f\"   Match baseline: {'âœ…' if match else 'âŒ'}\")\n",
    "    if not match:\n",
    "        print(f\"   Answer: {answer[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>computed</th>\n",
       "      <th>skipped</th>\n",
       "      <th>skip_rate</th>\n",
       "      <th>match_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>âœ…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993</td>\n",
       "      <td>993</td>\n",
       "      <td>31</td>\n",
       "      <td>3.0%</td>\n",
       "      <td>âœ…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.989</td>\n",
       "      <td>938</td>\n",
       "      <td>86</td>\n",
       "      <td>8.4%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985</td>\n",
       "      <td>899</td>\n",
       "      <td>125</td>\n",
       "      <td>12.2%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.981</td>\n",
       "      <td>851</td>\n",
       "      <td>173</td>\n",
       "      <td>16.9%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.978</td>\n",
       "      <td>682</td>\n",
       "      <td>150</td>\n",
       "      <td>18.0%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.974</td>\n",
       "      <td>828</td>\n",
       "      <td>196</td>\n",
       "      <td>19.1%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.970</td>\n",
       "      <td>817</td>\n",
       "      <td>207</td>\n",
       "      <td>20.2%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.966</td>\n",
       "      <td>440</td>\n",
       "      <td>104</td>\n",
       "      <td>19.1%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.962</td>\n",
       "      <td>431</td>\n",
       "      <td>113</td>\n",
       "      <td>20.8%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.958</td>\n",
       "      <td>354</td>\n",
       "      <td>94</td>\n",
       "      <td>21.0%</td>\n",
       "      <td>âŒ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  computed  skipped skip_rate match_baseline\n",
       "0       1.000      1024        0      0.0%              âœ…\n",
       "1       0.993       993       31      3.0%              âœ…\n",
       "2       0.989       938       86      8.4%              âŒ\n",
       "3       0.985       899      125     12.2%              âŒ\n",
       "4       0.981       851      173     16.9%              âŒ\n",
       "5       0.978       682      150     18.0%              âŒ\n",
       "6       0.974       828      196     19.1%              âŒ\n",
       "7       0.970       817      207     20.2%              âŒ\n",
       "8       0.966       440      104     19.1%              âŒ\n",
       "9       0.962       431      113     20.8%              âŒ\n",
       "10      0.958       354       94     21.0%              âŒ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['skip_rate'] = df['skip_rate'].apply(lambda x: f\"{x*100:.1f}%\")\n",
    "df['match_baseline'] = df['match_baseline'].apply(lambda x: 'âœ…' if x else 'âŒ')\n",
    "df['answer'] = df['answer'].apply(lambda x: x[:40] + '...' if len(x) > 40 else x)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "display(df[['threshold', 'computed', 'skipped', 'skip_rate', 'match_baseline']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç»“è®º\n",
    "\n",
    "1. **Sanity Check**: `layer_skip_threshold=1.0` æ—¶ï¼Œç»“æœä¸ `generate_with_dual_cache` å®Œå…¨ä¸€è‡´\n",
    "2. **Layer Skip æ•ˆæœ**: éšç€ threshold é™ä½ï¼Œæ›´å¤šå±‚è¢«è·³è¿‡ï¼Œä½†å¯èƒ½å½±å“ç”Ÿæˆè´¨é‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
