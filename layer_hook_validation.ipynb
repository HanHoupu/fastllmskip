{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Hook éªŒè¯å®éªŒ\n",
    "\n",
    "æœ¬ notebook éªŒè¯ `generate_with_dual_cache_hooked` ä¸åŸç‰ˆ `generate_with_dual_cache` çš„ç”Ÿæˆç»“æœå®Œå…¨ä¸€è‡´ã€‚\n",
    "\n",
    "**éªŒè¯ç›®æ ‡ï¼š**\n",
    "1. æ·»åŠ  hook åï¼Œç”Ÿæˆç»“æœä¸åŸç‰ˆå®Œå…¨ç›¸åŒ\n",
    "2. Hook èƒ½å¤Ÿæ­£ç¡®æ”¶é›†å„å±‚çš„ hidden states\n",
    "3. NFE (Number of Forward Evaluations) ä¿æŒä¸€è‡´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090 Ti\n",
      "Total VRAM: 23.99 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# è®¾ç½® GPUï¼ˆæ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹ï¼‰\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# åˆ‡æ¢åˆ° llada ç›®å½•\n",
    "os.chdir('llada')\n",
    "\n",
    "# æ¸…ç† GPU ç¼“å­˜\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pianng/miniconda3/envs/dllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: GSAI-ML/LLaDA-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded!\n",
      "\n",
      "Model info:\n",
      "   Number of layers: 32\n",
      "   Hidden size: 4096\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model.modeling_llada import LLaDAModelLM\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"âœ… Model loaded!\")\n",
    "\n",
    "# æ˜¾ç¤ºæ¨¡å‹ç»“æ„ä¿¡æ¯\n",
    "n_layers = model.model.config.n_layers\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"   Number of layers: {n_layers}\")\n",
    "print(f\"   Hidden size: {model.model.config.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å¯¼å…¥ç”Ÿæˆå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Functions imported!\n"
     ]
    }
   ],
   "source": [
    "from generate import (\n",
    "    generate_with_dual_cache,\n",
    "    generate_with_dual_cache_hooked,\n",
    "    LayerHiddenStatesCollector\n",
    ")\n",
    "\n",
    "print(\"âœ… Functions imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. éªŒè¯å®éªŒï¼šå¯¹æ¯”åŸç‰ˆä¸ Hook ç‰ˆæœ¬\n",
    "\n",
    "ä½¿ç”¨å¤šä¸ªä¸åŒçš„ prompt è¿›è¡Œæµ‹è¯•ï¼Œç¡®ä¿ç»“æœä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(prompt_text, tokenizer, device):\n",
    "    \"\"\"å‡†å¤‡è¾“å…¥ tensor\"\"\"\n",
    "    m = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    input_ids = tokenizer(prompt)['input_ids']\n",
    "    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    return input_ids\n",
    "\n",
    "def compare_generation(model, tokenizer, prompt_text, gen_params, device='cuda'):\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”åŸç‰ˆå’Œ hook ç‰ˆæœ¬çš„ç”Ÿæˆç»“æœã€‚\n",
    "    \n",
    "    Returns:\n",
    "        dict: åŒ…å«å¯¹æ¯”ç»“æœ\n",
    "    \"\"\"\n",
    "    input_ids = prepare_input(prompt_text, tokenizer, device)\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    \n",
    "    # 1. åŸç‰ˆç”Ÿæˆ\n",
    "    x_orig, nfe_orig = generate_with_dual_cache(\n",
    "        model, input_ids, **gen_params\n",
    "    )\n",
    "    answer_orig = tokenizer.decode(x_orig[0, prompt_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # 2. Hook ç‰ˆæœ¬ï¼ˆä¸å¯ç”¨ collectorï¼‰\n",
    "    x_hooked_no_collector, nfe_hooked_no = generate_with_dual_cache_hooked(\n",
    "        model, input_ids, collector=None, **gen_params\n",
    "    )\n",
    "    answer_hooked_no = tokenizer.decode(x_hooked_no_collector[0, prompt_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # 3. Hook ç‰ˆæœ¬ï¼ˆå¯ç”¨ collectorï¼‰\n",
    "    collector = LayerHiddenStatesCollector()\n",
    "    n_layers = collector.register_hooks(model)\n",
    "    \n",
    "    x_hooked, nfe_hooked, step_states = generate_with_dual_cache_hooked(\n",
    "        model, input_ids, collector=collector, collect_per_step=False, **gen_params\n",
    "    )\n",
    "    answer_hooked = tokenizer.decode(x_hooked[0, prompt_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # è·å–æœ€åä¸€æ­¥çš„ hidden states ä¿¡æ¯\n",
    "    layer_data = collector.get_layer_hidden_states()\n",
    "    collector.remove_hooks()\n",
    "    \n",
    "    # å¯¹æ¯”\n",
    "    tokens_match_no_collector = torch.equal(x_orig, x_hooked_no_collector)\n",
    "    tokens_match_with_collector = torch.equal(x_orig, x_hooked)\n",
    "    nfe_match = (nfe_orig == nfe_hooked_no == nfe_hooked)\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt_text,\n",
    "        'answer_orig': answer_orig,\n",
    "        'answer_hooked_no_collector': answer_hooked_no,\n",
    "        'answer_hooked_with_collector': answer_hooked,\n",
    "        'nfe_orig': nfe_orig,\n",
    "        'nfe_hooked_no_collector': nfe_hooked_no,\n",
    "        'nfe_hooked_with_collector': nfe_hooked,\n",
    "        'tokens_match_no_collector': tokens_match_no_collector,\n",
    "        'tokens_match_with_collector': tokens_match_with_collector,\n",
    "        'nfe_match': nfe_match,\n",
    "        'n_layers_hooked': n_layers,\n",
    "        'layer_inputs_collected': len(layer_data['inputs']),\n",
    "        'layer_outputs_collected': len(layer_data['outputs']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹éªŒè¯å®éªŒ...\n",
      "================================================================================\n",
      "\n",
      "[Test 1/4] Prompt: What is 2 + 2?...\n",
      "   âœ… PASSED\n",
      "   Tokens match (no collector): True\n",
      "   Tokens match (with collector): True\n",
      "   NFE: orig=5, hooked_no=5, hooked=5\n",
      "   Layers hooked: 32\n",
      "   Hidden states collected: inputs=32, outputs=32\n",
      "\n",
      "[Test 2/4] Prompt: Who is Newton?...\n",
      "   âœ… PASSED\n",
      "   Tokens match (no collector): True\n",
      "   Tokens match (with collector): True\n",
      "   NFE: orig=17, hooked_no=17, hooked=17\n",
      "   Layers hooked: 32\n",
      "   Hidden states collected: inputs=32, outputs=32\n",
      "\n",
      "[Test 3/4] Prompt: Lily can run 12 km/h for 4 hours. After that, she ...\n",
      "   âœ… PASSED\n",
      "   Tokens match (no collector): True\n",
      "   Tokens match (with collector): True\n",
      "   NFE: orig=32, hooked_no=32, hooked=32\n",
      "   Layers hooked: 32\n",
      "   Hidden states collected: inputs=32, outputs=32\n",
      "\n",
      "[Test 4/4] Prompt: Explain the concept of machine learning in one sen...\n",
      "   âœ… PASSED\n",
      "   Tokens match (no collector): True\n",
      "   Tokens match (with collector): True\n",
      "   NFE: orig=23, hooked_no=23, hooked=23\n",
      "   Layers hooked: 32\n",
      "   Hidden states collected: inputs=32, outputs=32\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Hook ç‰ˆæœ¬ä¸åŸç‰ˆç”Ÿæˆç»“æœå®Œå…¨ä¸€è‡´ã€‚\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ç”¨çš„ prompts\n",
    "test_prompts = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Who is Newton?\",\n",
    "    \"Lily can run 12 km/h for 4 hours. After that, she runs 6 km/h. How many km can she run in 8 hours?\",\n",
    "    \"Explain the concept of machine learning in one sentence.\",\n",
    "]\n",
    "\n",
    "# ç”Ÿæˆå‚æ•°\n",
    "gen_params = {\n",
    "    'steps': 32,\n",
    "    'gen_length': 64,\n",
    "    'block_length': 32,\n",
    "    'temperature': 0.,\n",
    "    'remasking': 'low_confidence',\n",
    "    'threshold': 0.9,\n",
    "}\n",
    "\n",
    "print(\"å¼€å§‹éªŒè¯å®éªŒ...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_results = []\n",
    "all_passed = True\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n[Test {i+1}/{len(test_prompts)}] Prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    result = compare_generation(model, tokenizer, prompt, gen_params)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # æ£€æŸ¥ç»“æœ\n",
    "    passed = result['tokens_match_no_collector'] and result['tokens_match_with_collector'] and result['nfe_match']\n",
    "    all_passed = all_passed and passed\n",
    "    \n",
    "    status = \"âœ… PASSED\" if passed else \"âŒ FAILED\"\n",
    "    print(f\"   {status}\")\n",
    "    print(f\"   Tokens match (no collector): {result['tokens_match_no_collector']}\")\n",
    "    print(f\"   Tokens match (with collector): {result['tokens_match_with_collector']}\")\n",
    "    print(f\"   NFE: orig={result['nfe_orig']}, hooked_no={result['nfe_hooked_no_collector']}, hooked={result['nfe_hooked_with_collector']}\")\n",
    "    print(f\"   Layers hooked: {result['n_layers_hooked']}\")\n",
    "    print(f\"   Hidden states collected: inputs={result['layer_inputs_collected']}, outputs={result['layer_outputs_collected']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_passed:\n",
    "    print(\"ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Hook ç‰ˆæœ¬ä¸åŸç‰ˆç”Ÿæˆç»“æœå®Œå…¨ä¸€è‡´ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è¯¦ç»†å¯¹æ¯”è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>NFE (orig)</th>\n",
       "      <th>NFE (hooked)</th>\n",
       "      <th>Tokens Match</th>\n",
       "      <th>Layers Hooked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is 2 + 2?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is Newton?</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lily can run 12 km/h for 4 hours. After ...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the concept of machine learning ...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Prompt  NFE (orig)  NFE (hooked)  \\\n",
       "0                               What is 2 + 2?           5             5   \n",
       "1                               Who is Newton?          17            17   \n",
       "2  Lily can run 12 km/h for 4 hours. After ...          32            32   \n",
       "3  Explain the concept of machine learning ...          23            23   \n",
       "\n",
       "  Tokens Match  Layers Hooked  \n",
       "0            âœ…             32  \n",
       "1            âœ…             32  \n",
       "2            âœ…             32  \n",
       "3            âœ…             32  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºå¯¹æ¯”è¡¨æ ¼\n",
    "comparison_data = []\n",
    "for r in all_results:\n",
    "    comparison_data.append({\n",
    "        'Prompt': r['prompt'][:40] + '...' if len(r['prompt']) > 40 else r['prompt'],\n",
    "        'NFE (orig)': r['nfe_orig'],\n",
    "        'NFE (hooked)': r['nfe_hooked_with_collector'],\n",
    "        'Tokens Match': 'âœ…' if r['tokens_match_with_collector'] else 'âŒ',\n",
    "        'Layers Hooked': r['n_layers_hooked'],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. éªŒè¯ Hidden States æ”¶é›†\n",
    "\n",
    "ç¡®è®¤ collector æ­£ç¡®æ”¶é›†äº†å„å±‚çš„ hidden statesã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered hooks on 32 layers\n",
      "\n",
      "Collected hidden states:\n",
      "   Number of layer inputs: 32\n",
      "   Number of layer outputs: 32\n",
      "\n",
      "   Sample tensor shape (layer 0): torch.Size([1, 32, 4096])\n",
      "   dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ prompt è¿›è¡Œè¯¦ç»†æ£€æŸ¥\n",
    "test_prompt = \"What is 2 + 2?\"\n",
    "input_ids = prepare_input(test_prompt, tokenizer, device)\n",
    "\n",
    "# åˆ›å»º collector å¹¶æ³¨å†Œ hooks\n",
    "collector = LayerHiddenStatesCollector()\n",
    "n_layers = collector.register_hooks(model)\n",
    "\n",
    "print(f\"Registered hooks on {n_layers} layers\")\n",
    "\n",
    "# è¿è¡Œç”Ÿæˆ\n",
    "x, nfe, _ = generate_with_dual_cache_hooked(\n",
    "    model, input_ids, collector=collector, collect_per_step=False,\n",
    "    steps=16, gen_length=32, block_length=32, temperature=0., threshold=0.9\n",
    ")\n",
    "\n",
    "# è·å– hidden states\n",
    "layer_data = collector.get_layer_hidden_states()\n",
    "\n",
    "print(f\"\\nCollected hidden states:\")\n",
    "print(f\"   Number of layer inputs: {len(layer_data['inputs'])}\")\n",
    "print(f\"   Number of layer outputs: {len(layer_data['outputs'])}\")\n",
    "\n",
    "# æ£€æŸ¥å½¢çŠ¶\n",
    "if layer_data['outputs']:\n",
    "    sample_layer = list(layer_data['outputs'].keys())[0]\n",
    "    sample_tensor = layer_data['outputs'][sample_layer]\n",
    "    print(f\"\\n   Sample tensor shape (layer {sample_layer}): {sample_tensor.shape}\")\n",
    "    print(f\"   dtype: {sample_tensor.dtype}\")\n",
    "\n",
    "collector.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„å±‚ Hidden States ç»Ÿè®¡ä¿¡æ¯:\n",
      "------------------------------------------------------------\n",
      "Layer  0: mean=-0.0118, std=0.5781, min=-34.5000, max=+2.1250\n",
      "Layer  1: mean=-0.0152, std=0.6690, min=-40.2500, max=+2.1719\n",
      "Layer  2: mean=-0.0165, std=0.7223, min=-42.7500, max=+2.0469\n",
      "Layer  3: mean=-0.0178, std=0.8159, min=-47.2500, max=+2.3906\n",
      "Layer  4: mean=-0.0161, std=0.8788, min=-51.2500, max=+3.0000\n",
      "Layer  5: mean=-0.0201, std=0.9626, min=-55.7500, max=+3.0938\n",
      "Layer  6: mean=-0.0204, std=1.0173, min=-61.0000, max=+3.8906\n",
      "Layer  7: mean=-0.0205, std=1.0437, min=-63.5000, max=+3.9844\n",
      "Layer  8: mean=-0.0203, std=1.0534, min=-62.2500, max=+4.2500\n",
      "Layer  9: mean=-0.0212, std=1.1357, min=-68.0000, max=+5.0938\n",
      "Layer 10: mean=-0.0221, std=1.1500, min=-71.5000, max=+5.5938\n",
      "Layer 11: mean=-0.0209, std=1.1531, min=-74.0000, max=+6.7188\n",
      "Layer 12: mean=-0.0235, std=1.2016, min=-82.0000, max=+7.6875\n",
      "Layer 13: mean=-0.0228, std=1.2805, min=-86.0000, max=+8.5000\n",
      "Layer 14: mean=-0.0244, std=1.3256, min=-97.5000, max=+8.8125\n",
      "Layer 15: mean=-0.0298, std=1.3642, min=-92.0000, max=+9.6250\n",
      "Layer 16: mean=-0.0379, std=1.5126, min=-95.5000, max=+10.4375\n",
      "Layer 17: mean=-0.0369, std=1.6297, min=-99.0000, max=+12.6875\n",
      "Layer 18: mean=-0.0390, std=1.7693, min=-110.5000, max=+15.8750\n",
      "Layer 19: mean=-0.0422, std=1.8006, min=-107.0000, max=+17.0000\n",
      "Layer 20: mean=-0.0468, std=1.9123, min=-116.5000, max=+18.7500\n",
      "Layer 21: mean=-0.0443, std=2.1559, min=-129.0000, max=+21.0000\n",
      "Layer 22: mean=-0.0384, std=2.2135, min=-125.0000, max=+24.7500\n",
      "Layer 23: mean=-0.0362, std=2.2865, min=-119.0000, max=+27.8750\n",
      "Layer 24: mean=-0.0148, std=2.4797, min=-124.0000, max=+33.5000\n",
      "Layer 25: mean=+0.0060, std=2.8249, min=-125.0000, max=+40.5000\n",
      "Layer 26: mean=+0.0129, std=3.2445, min=-141.0000, max=+43.5000\n",
      "Layer 27: mean=+0.0019, std=3.9703, min=-180.0000, max=+45.7500\n",
      "Layer 28: mean=-0.0007, std=4.6162, min=-233.0000, max=+45.5000\n",
      "Layer 29: mean=-0.0031, std=5.3288, min=-270.0000, max=+69.5000\n",
      "Layer 30: mean=-0.0023, std=6.0711, min=-344.0000, max=+92.5000\n",
      "Layer 31: mean=-0.0011, std=7.4837, min=-446.0000, max=+190.0000\n"
     ]
    }
   ],
   "source": [
    "# æ˜¾ç¤ºå„å±‚ hidden states çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"å„å±‚ Hidden States ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for layer_idx in sorted(layer_data['outputs'].keys()):\n",
    "    tensor = layer_data['outputs'][layer_idx]\n",
    "    tensor_float = tensor.float()  # è½¬æ¢ä¸º float ä»¥ä¾¿è®¡ç®—ç»Ÿè®¡å€¼\n",
    "    \n",
    "    mean_val = tensor_float.mean().item()\n",
    "    std_val = tensor_float.std().item()\n",
    "    min_val = tensor_float.min().item()\n",
    "    max_val = tensor_float.max().item()\n",
    "    \n",
    "    print(f\"Layer {layer_idx:2d}: mean={mean_val:+.4f}, std={std_val:.4f}, min={min_val:+.4f}, max={max_val:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç»“è®º\n",
    "\n",
    "æœ¬ notebook éªŒè¯äº†ï¼š\n",
    "\n",
    "1. **ç”Ÿæˆç»“æœä¸€è‡´æ€§**: `generate_with_dual_cache_hooked` åœ¨å¯ç”¨å’Œä¸å¯ç”¨ collector æ—¶ï¼Œä¸åŸç‰ˆ `generate_with_dual_cache` ç”Ÿæˆçš„ token åºåˆ—å®Œå…¨ç›¸åŒ\n",
    "\n",
    "2. **NFE ä¸€è‡´æ€§**: å‰å‘ä¼ æ’­æ¬¡æ•°ä¿æŒä¸€è‡´\n",
    "\n",
    "3. **Hidden States æ”¶é›†**: Collector èƒ½å¤Ÿæ­£ç¡®æ”¶é›†æ‰€æœ‰å±‚çš„è¾“å…¥å’Œè¾“å‡º hidden states\n",
    "\n",
    "è¿™ä¸ºåç»­çš„ layer-level similarity åˆ†ææä¾›äº†å¯é çš„åŸºç¡€ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
