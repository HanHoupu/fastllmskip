{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mid-Layer Early-Exit 评测 Notebook\n",
        "\n",
        "在 GSM8K 上评测 **Dual Cache + Mid-Layer Early-Exit** 的效果。\n",
        "\n",
        "**工作原理：**\n",
        "- 在每个 refinement step，先计算到第 k 层\n",
        "- 比较当前 hidden state 与上一步的 cosine similarity\n",
        "- 如果相似度高于阈值，跳过后续层计算\n",
        "\n",
        "**实验内容:**\n",
        "1. Baseline: Dual Cache（不使用 early-exit）\n",
        "2. Dual Cache + 不同 early_exit_layer (16, 20, 24, 26)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Set GPU (modify as needed)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
        "\n",
        "# Environment settings\n",
        "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
        "os.environ['HF_ALLOW_CODE_EVAL'] = '1'\n",
        "os.environ['HF_DATASETS_TRUST_REMOTE_CODE'] = 'true'\n",
        "\n",
        "# Change to llada directory\n",
        "os.chdir('llada')\n",
        "\n",
        "# Create log directory\n",
        "os.makedirs('nlogs', exist_ok=True)\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 快速验证: Baseline vs Mid-Layer Skip (limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick test: only evaluate 50 samples\n",
        "import subprocess\n",
        "import datetime\n",
        "\n",
        "task = \"gsm8k\"\n",
        "fewshot = 5\n",
        "limit = 50  # Only test 50 samples\n",
        "gpu = 0  # GPU ID\n",
        "\n",
        "# Configuration: (early_exit_layer, name)\n",
        "# None means no early-exit (baseline)\n",
        "experiments = [\n",
        "    (None, \"baseline\"),           # Baseline\n",
        "    (24, \"midskip_L24\"),          # Early-exit at Layer 24\n",
        "]\n",
        "\n",
        "threshold = 0.9  # Fixed threshold\n",
        "\n",
        "for exit_layer, name in experiments:\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = f\"nlogs/{task}_{name}_{timestamp}.log\"\n",
        "    \n",
        "    # Build model_args string (correct format for lm-eval)\n",
        "    # IMPORTANT: steps should be >= gen_length for good quality!\n",
        "    # steps_per_block = steps / num_blocks = steps / (gen_length / block_length)\n",
        "    # For gen_length=256, block_length=32: num_blocks=8\n",
        "    # If steps=256: steps_per_block=32 (good)\n",
        "    # If steps=32:  steps_per_block=4  (too few, bad quality!)\n",
        "    model_args = [\n",
        "        \"model_path='GSAI-ML/LLaDA-8B-Instruct'\",\n",
        "        \"gen_length=256\",\n",
        "        \"steps=256\",  # FIXED: was 32, should be 256 for good quality\n",
        "        \"block_length=32\",\n",
        "        \"threshold=0.9\",\n",
        "        \"use_cache=True\",\n",
        "        \"dual_cache=True\",\n",
        "        \"show_speed=True\",\n",
        "    ]\n",
        "    \n",
        "    if exit_layer is not None:\n",
        "        model_args.extend([\n",
        "            \"mid_layer_skip=True\",\n",
        "            f\"early_exit_layer={exit_layer}\",\n",
        "            f\"early_exit_threshold={threshold}\",\n",
        "        ])\n",
        "    else:\n",
        "        model_args.append(\"mid_layer_skip=False\")\n",
        "    \n",
        "    # Correct command format using accelerate launch and --model_args\n",
        "    cmd = f\"\"\"CUDA_VISIBLE_DEVICES={gpu} accelerate launch eval_llada.py \\\n",
        "        --tasks {task} --num_fewshot {fewshot} --limit {limit} \\\n",
        "        --confirm_run_unsafe_code --model llada_dist \\\n",
        "        --model_args {','.join(model_args)}\"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running: {name}\")\n",
        "    print(f\"Command: {cmd[:200]}...\")\n",
        "    print(f\"Log file: {log_file}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    with open(log_file, 'w') as f:\n",
        "        result = subprocess.run(cmd, shell=True, stdout=f, stderr=subprocess.STDOUT, text=True)\n",
        "    \n",
        "    # Show last few lines of log\n",
        "    with open(log_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        print(\"Last 10 lines of output:\")\n",
        "        for line in lines[-10:]:\n",
        "            print(line.rstrip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 完整测试: 不同 Early-Exit Layer 并行运行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import datetime\n",
        "\n",
        "task = \"gsm8k\"\n",
        "fewshot = 4\n",
        "threshold = 0.9  # Fixed threshold\n",
        "\n",
        "# Configuration: (early_exit_layer, gpu, name)\n",
        "# None means baseline\n",
        "configs = [\n",
        "    (None, 0, \"baseline\"),\n",
        "    (16, 1, \"midskip_L16\"),\n",
        "    (20, 2, \"midskip_L20\"),\n",
        "    (24, 3, \"midskip_L24\"),\n",
        "    (26, 4, \"midskip_L26\"),\n",
        "]\n",
        "\n",
        "processes = []\n",
        "\n",
        "for early_exit_layer, gpu, name in configs:\n",
        "    log_file = f\"nlogs/midskip_{task}_{name}_{datetime.datetime.now():%F_%H-%M-%S}.log\"\n",
        "    \n",
        "    # Build arguments\n",
        "    skip_args = f\"dual_cache=True\"\n",
        "    if early_exit_layer is not None:\n",
        "        skip_args += f\",mid_layer_skip=True,early_exit_layer={early_exit_layer},early_exit_threshold={threshold}\"\n",
        "    else:\n",
        "        skip_args += \",mid_layer_skip=False\"\n",
        "    \n",
        "    # FIXED: steps should be >= gen_length for good quality\n",
        "    # gen_length=256, steps=256, block_length=32 -> steps_per_block=32 (good)\n",
        "    cmd = f\"\"\"CUDA_VISIBLE_DEVICES={gpu} accelerate launch eval_llada.py \\\n",
        "        --tasks {task} --num_fewshot {fewshot} \\\n",
        "        --confirm_run_unsafe_code --model llada_dist \\\n",
        "        --model_args model_path='GSAI-ML/LLaDA-8B-Instruct',gen_length=256,steps=256,block_length=32,threshold=0.9,{skip_args},show_speed=True \\\n",
        "        --output_path evals_results/midskip/gsm8k-{name} --log_samples\"\"\"\n",
        "    \n",
        "    print(f\"GPU{gpu} running: {name}\")\n",
        "    p = subprocess.Popen(cmd, shell=True, stdout=open(log_file, \"w\"), stderr=subprocess.STDOUT)\n",
        "    processes.append((p, name, log_file))\n",
        "\n",
        "print(f\"\\n{len(processes)} tasks launched in parallel, waiting...\")\n",
        "\n",
        "for p, name, log in processes:\n",
        "    p.wait()\n",
        "    print(f\"{name} finished (exit code: {p.returncode})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 查看评测结果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import glob\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Find recent log files and extract key metrics\n",
        "log_files = sorted(glob.glob(\"nlogs/midskip_gsm8k*.log\"), key=os.path.getmtime, reverse=True)\n",
        "\n",
        "print(\"Mid-Layer Skip GSM8K Results:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "for log_file in log_files[:10]:  # Most recent 10\n",
        "    name = os.path.basename(log_file)\n",
        "    \n",
        "    with open(log_file, 'r') as f:\n",
        "        content = f.read()\n",
        "        \n",
        "    # Extract accuracy\n",
        "    acc_match = re.search(r'exact_match[,:\\|]?\\s*([\\d.]+)', content)\n",
        "    acc = float(acc_match.group(1)) * 100 if acc_match else None\n",
        "    \n",
        "    # Extract speed info\n",
        "    speed_match = re.search(r'Tokens per second:\\s*([\\d.]+)', content)\n",
        "    speed = float(speed_match.group(1)) if speed_match else None\n",
        "    \n",
        "    # Extract NFE\n",
        "    nfe_match = re.search(r'Total NFE is (\\d+)', content)\n",
        "    nfe = int(nfe_match.group(1)) if nfe_match else None\n",
        "    \n",
        "    # Extract time\n",
        "    time_match = re.search(r'Total time taken:\\s*([\\d.]+)', content)\n",
        "    time_sec = float(time_match.group(1)) if time_match else None\n",
        "    \n",
        "    # Extract early-exit info\n",
        "    ee_match = re.search(r'Early-Exit rate:\\s*([\\d.]+)%', content)\n",
        "    ee_rate = float(ee_match.group(1)) if ee_match else None\n",
        "    \n",
        "    results.append({\n",
        "        'name': name,\n",
        "        'accuracy': acc,\n",
        "        'tokens_per_sec': speed,\n",
        "        'total_nfe': nfe,\n",
        "        'time_sec': time_sec,\n",
        "        'early_exit_rate': ee_rate,\n",
        "    })\n",
        "\n",
        "# Formatted output\n",
        "print(f\"{'Name':<55} {'Acc%':<8} {'Tok/s':<10} {'NFE':<10} {'Time(s)':<10} {'EE%':<8}\")\n",
        "print(\"-\" * 80)\n",
        "for r in results:\n",
        "    acc_str = f\"{r['accuracy']:.2f}\" if r['accuracy'] else \"N/A\"\n",
        "    speed_str = f\"{r['tokens_per_sec']:.1f}\" if r['tokens_per_sec'] else \"N/A\"\n",
        "    nfe_str = str(r['total_nfe']) if r['total_nfe'] else \"N/A\"\n",
        "    time_str = f\"{r['time_sec']:.1f}\" if r['time_sec'] else \"N/A\"\n",
        "    ee_str = f\"{r['early_exit_rate']:.1f}\" if r['early_exit_rate'] else \"N/A\"\n",
        "    print(f\"{r['name']:<55} {acc_str:<8} {speed_str:<10} {nfe_str:<10} {time_str:<10} {ee_str:<8}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 手动运行单个实验（可选）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# For running a single experiment manually\n",
        "import subprocess\n",
        "import datetime\n",
        "\n",
        "# Configuration\n",
        "early_exit_layer = 24  # Set to None for baseline\n",
        "threshold = 0.9\n",
        "limit = 100  # Number of samples\n",
        "gpu = 0\n",
        "\n",
        "name = f\"manual_L{early_exit_layer}\" if early_exit_layer else \"manual_baseline\"\n",
        "log_file = f\"nlogs/manual_midskip_{name}_{datetime.datetime.now():%F_%H-%M-%S}.log\"\n",
        "\n",
        "skip_args = f\"dual_cache=True\"\n",
        "if early_exit_layer is not None:\n",
        "    skip_args += f\",mid_layer_skip=True,early_exit_layer={early_exit_layer},early_exit_threshold={threshold}\"\n",
        "else:\n",
        "    skip_args += \",mid_layer_skip=False\"\n",
        "\n",
        "cmd = f\"\"\"CUDA_VISIBLE_DEVICES={gpu} accelerate launch eval_llada.py     --tasks gsm8k --num_fewshot 5 --limit {limit}     --confirm_run_unsafe_code --model llada_dist     --model_args model_path='GSAI-ML/LLaDA-8B-Instruct',gen_length=64,steps=16,block_length=8,threshold=0.9,{skip_args},show_speed=True     --output_path evals_results/midskip/manual-{name} --log_samples\"\"\"\n",
        "\n",
        "print(f\"Running: {name}\")\n",
        "print(f\"Log: {log_file}\")\n",
        "print(f\"Command: {cmd[:200]}...\")\n",
        "\n",
        "# Uncomment below to run\n",
        "# p = subprocess.Popen(cmd, shell=True, stdout=open(log_file, \"w\"), stderr=subprocess.STDOUT)\n",
        "# p.wait()\n",
        "# print(f\"Finished (exit code: {p.returncode})\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}