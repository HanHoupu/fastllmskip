{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TokenSkip 全量中间变量追踪 Notebook\n",
        "\n",
        "用 Hook 抓取所有可能的中间变量，深度对比 baseline 和 tokenskip。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pianng/miniconda3/envs/dllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型加载完成，32 层\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "from model.modeling_llada import LLaDAModelLM\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda'\n",
        "model = LLaDAModelLM.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', torch_dtype=torch.bfloat16).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct')\n",
        "print(f\"模型加载完成，{len(model.model.transformer.blocks)} 层\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Run All 基础设置\n",
        "说明：本 notebook 建议直接 Run All。此处集中放全局 import / logging / debug 参数，避免后续单元报错或顺序依赖。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import warnings\n",
        "from generate import generate_with_dual_cache, generate_with_dual_cache_tokenskip, get_transfer_index, get_num_transfer_tokens\n",
        "\n",
        "# 统一屏蔽一个无害警告（避免 Run All 被噪音打断）\n",
        "warnings.filterwarnings(\"ignore\", message=\"To copy construct from a tensor\")\n",
        "\n",
        "# logging 仅初始化一次\n",
        "if not logging.getLogger().handlers:\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        "    )\n",
        "\n",
        "# 轻量调试参数（可按需调整）\n",
        "DEBUG_BLOCKS = 1  # 只追踪前几个 block\n",
        "DEBUG_STEPS = 8   # 每个 block 只追踪前几个 step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt_len: 19, total_len: 147\n"
          ]
        }
      ],
      "source": [
        "# 准备输入\n",
        "prompt = \"Who is Newton in physics?\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "input_ids = torch.tensor(tokenizer(text)['input_ids']).to(device).unsqueeze(0)\n",
        "\n",
        "MASK_ID = 126336\n",
        "GEN_LENGTH = 128\n",
        "BLOCK_LENGTH = 32\n",
        "\n",
        "prompt_len = input_ids.shape[1]\n",
        "total_len = prompt_len + GEN_LENGTH\n",
        "\n",
        "x_init = torch.full((1, total_len), MASK_ID, dtype=torch.long, device=device)\n",
        "x_init[:, :prompt_len] = input_ids\n",
        "\n",
        "print(f\"prompt_len: {prompt_len}, total_len: {total_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 全量 Tracer 类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FullTracer 定义完成\n"
          ]
        }
      ],
      "source": [
        "class FullTracer:\n",
        "    \"\"\"全量中间变量追踪器\"\"\"\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.hooks = []\n",
        "        self.traces = {}\n",
        "        self._register_hooks()\n",
        "    \n",
        "    def _register_hooks(self):\n",
        "        # 1. Embedding 层\n",
        "        self.hooks.append(\n",
        "            self.model.model.transformer.wte.register_forward_hook(\n",
        "                self._make_hook('embedding')\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # 2. 每个 Block 的详细追踪\n",
        "        for i, block in enumerate(self.model.model.transformer.blocks):\n",
        "            # Block 整体输入输出\n",
        "            self.hooks.append(\n",
        "                block.register_forward_hook(self._make_hook(f'block_{i}'))\n",
        "            )\n",
        "            \n",
        "            # Attention 前的 LayerNorm\n",
        "            self.hooks.append(\n",
        "                block.attn_norm.register_forward_hook(self._make_hook(f'block_{i}_attn_norm'))\n",
        "            )\n",
        "            \n",
        "            # FFN 前的 LayerNorm\n",
        "            self.hooks.append(\n",
        "                block.ff_norm.register_forward_hook(self._make_hook(f'block_{i}_ff_norm'))\n",
        "            )\n",
        "            \n",
        "            # Q/K/V 投影\n",
        "            self.hooks.append(\n",
        "                block.q_proj.register_forward_hook(self._make_hook(f'block_{i}_q_proj'))\n",
        "            )\n",
        "            self.hooks.append(\n",
        "                block.k_proj.register_forward_hook(self._make_hook(f'block_{i}_k_proj'))\n",
        "            )\n",
        "            self.hooks.append(\n",
        "                block.v_proj.register_forward_hook(self._make_hook(f'block_{i}_v_proj'))\n",
        "            )\n",
        "            \n",
        "            # Attention 输出投影\n",
        "            self.hooks.append(\n",
        "                block.attn_out.register_forward_hook(self._make_hook(f'block_{i}_attn_out'))\n",
        "            )\n",
        "            \n",
        "            # FFN 中间层\n",
        "            self.hooks.append(\n",
        "                block.ff_proj.register_forward_hook(self._make_hook(f'block_{i}_ff_proj'))\n",
        "            )\n",
        "            self.hooks.append(\n",
        "                block.up_proj.register_forward_hook(self._make_hook(f'block_{i}_up_proj'))\n",
        "            )\n",
        "            \n",
        "            # FFN 输出投影\n",
        "            self.hooks.append(\n",
        "                block.ff_out.register_forward_hook(self._make_hook(f'block_{i}_ffn_out'))\n",
        "            )\n",
        "            \n",
        "            # RoPE\n",
        "            self.hooks.append(\n",
        "                block.rotary_emb.register_forward_hook(self._make_hook(f'block_{i}_rope'))\n",
        "            )\n",
        "        \n",
        "        # 3. 最终 LayerNorm\n",
        "        self.hooks.append(\n",
        "            self.model.model.transformer.ln_f.register_forward_hook(\n",
        "                self._make_hook('final_norm')\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    def _make_hook(self, name):\n",
        "        def hook(module, input, output):\n",
        "            inp = input[0] if isinstance(input, tuple) and len(input) > 0 else input\n",
        "            out = output[0] if isinstance(output, tuple) and len(output) > 0 else output\n",
        "            \n",
        "            self.traces[name] = {\n",
        "                'input_shape': inp.shape if hasattr(inp, 'shape') else None,\n",
        "                'output_shape': out.shape if hasattr(out, 'shape') else None,\n",
        "                'input': inp.clone().detach() if hasattr(inp, 'clone') else inp,\n",
        "                'output': out.clone().detach() if hasattr(out, 'clone') else out,\n",
        "            }\n",
        "            \n",
        "            if hasattr(out, 'float'):\n",
        "                out_f = out.float()\n",
        "                self.traces[name]['stats'] = {\n",
        "                    'mean': out_f.mean().item(),\n",
        "                    'std': out_f.std().item(),\n",
        "                    'min': out_f.min().item(),\n",
        "                    'max': out_f.max().item(),\n",
        "                    'abs_mean': out_f.abs().mean().item(),\n",
        "                }\n",
        "        return hook\n",
        "    \n",
        "    def clear(self):\n",
        "        self.traces = {}\n",
        "    \n",
        "    def remove(self):\n",
        "        for h in self.hooks:\n",
        "            h.remove()\n",
        "        self.hooks = []\n",
        "\n",
        "print(\"FullTracer 定义完成\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 初始化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "初始 forward 完成，KV cache: 32 层\n",
            "KV shape: torch.Size([1, 32, 147, 128])\n"
          ]
        }
      ],
      "source": [
        "# 初始 forward\n",
        "x = x_init.clone()\n",
        "with torch.no_grad():\n",
        "    out_init = model(x, use_cache=True, output_hidden_states=True)\n",
        "\n",
        "past_kv = out_init.past_key_values\n",
        "\n",
        "# 初始采样\n",
        "x0 = out_init.logits.argmax(dim=-1)\n",
        "mask_pos = (x == MASK_ID)\n",
        "x = torch.where(mask_pos, x0, x)\n",
        "\n",
        "print(f\"初始 forward 完成，KV cache: {len(past_kv)} 层\")\n",
        "print(f\"KV shape: {past_kv[0][0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block [19, 51)\n",
            "超参: K=8, threshold=0.95, outlier=0.7\n"
          ]
        }
      ],
      "source": [
        "# 定义 block 和超参\n",
        "s = prompt_len\n",
        "e = s + BLOCK_LENGTH\n",
        "\n",
        "replace_position = torch.zeros_like(x, dtype=torch.bool)\n",
        "replace_position[:, s:e] = True\n",
        "\n",
        "SKIP_LAYER_K = 8\n",
        "SKIP_THRESHOLD = 0.95\n",
        "SKIP_OUTLIER = 0.7\n",
        "\n",
        "print(f\"Block [{s}, {e})\")\n",
        "print(f\"超参: K={SKIP_LAYER_K}, threshold={SKIP_THRESHOLD}, outlier={SKIP_OUTLIER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Baseline vs TokenSkip（prev_hidden=None）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Step 0: logits shape torch.Size([1, 32, 126464])\n"
          ]
        }
      ],
      "source": [
        "tracer = FullTracer(model)\n",
        "\n",
        "# Step 0: Baseline\n",
        "x_baseline = x.clone()\n",
        "past_kv_baseline = tuple((k.clone(), v.clone()) for k, v in past_kv)\n",
        "\n",
        "tracer.clear()\n",
        "with torch.no_grad():\n",
        "    out0_baseline = model(\n",
        "        x_baseline[:, s:e],\n",
        "        past_key_values=past_kv_baseline,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_position.clone(),\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "traces_baseline_0 = dict(tracer.traces)\n",
        "print(f\"Baseline Step 0: logits shape {out0_baseline.logits.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TokenSkip Step 0: logits shape torch.Size([1, 32, 126464])\n"
          ]
        }
      ],
      "source": [
        "# Step 0: TokenSkip（prev_hidden=None）\n",
        "x_skip = x.clone()\n",
        "past_kv_skip = tuple((k.clone(), v.clone()) for k, v in past_kv)\n",
        "\n",
        "tracer.clear()\n",
        "with torch.no_grad():\n",
        "    out0_skip = model(\n",
        "        x_skip[:, s:e],\n",
        "        past_key_values=past_kv_skip,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_position.clone(),\n",
        "        output_hidden_states=True,\n",
        "        skip_layer_k=SKIP_LAYER_K,\n",
        "        skip_threshold=SKIP_THRESHOLD,\n",
        "        skip_outlier=SKIP_OUTLIER,\n",
        "        prev_hidden=None\n",
        "    )\n",
        "traces_skip_0 = dict(tracer.traces)\n",
        "print(f\"TokenSkip Step 0: logits shape {out0_skip.logits.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 logits diff: 0.000000\n",
            "预期: 0（prev_hidden=None 不 skip）\n",
            "结果: ✓ PASS\n"
          ]
        }
      ],
      "source": [
        "# 对比 Step 0\n",
        "diff0_logits = (out0_baseline.logits - out0_skip.logits).abs().max().item()\n",
        "print(f\"Step 0 logits diff: {diff0_logits:.6f}\")\n",
        "print(f\"预期: 0（prev_hidden=None 不 skip）\")\n",
        "print(f\"结果: {'✓ PASS' if diff0_logits == 0 else '✗ FAIL'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 逐层 block 输出对比:\n"
          ]
        }
      ],
      "source": [
        "# 对比每层 block 输出\n",
        "print(\"Step 0 逐层 block 输出对比:\")\n",
        "for i in range(32):\n",
        "    key = f'block_{i}'\n",
        "    if key in traces_baseline_0 and key in traces_skip_0:\n",
        "        b_out = traces_baseline_0[key]['output']\n",
        "        s_out = traces_skip_0[key]['output']\n",
        "        if b_out.shape == s_out.shape:\n",
        "            diff = (b_out - s_out).abs().max().item()\n",
        "            if diff > 0:\n",
        "                print(f\"  Layer {i}: diff={diff:.6f} ***\")\n",
        "        else:\n",
        "            print(f\"  Layer {i}: SHAPE MISMATCH {b_out.shape} vs {s_out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prev_hidden: 33 层\n"
          ]
        }
      ],
      "source": [
        "# 采样更新 x\n",
        "pred0 = out0_baseline.logits.argmax(dim=-1)\n",
        "mask_blk = (x_baseline[:, s:e] == MASK_ID)\n",
        "x_baseline[:, s:e] = torch.where(mask_blk, pred0, x_baseline[:, s:e])\n",
        "x_skip[:, s:e] = torch.where(mask_blk, pred0, x_skip[:, s:e])\n",
        "\n",
        "prev_hidden = out0_skip.hidden_states\n",
        "print(f\"prev_hidden: {len(prev_hidden)} 层\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: 关键对比（有 prev_hidden）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Step 1 完成\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Baseline\n",
        "tracer.clear()\n",
        "with torch.no_grad():\n",
        "    out1_baseline = model(\n",
        "        x_baseline[:, s:e],\n",
        "        past_key_values=out0_baseline.past_key_values,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_position.clone(),\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "traces_baseline_1 = dict(tracer.traces)\n",
        "print(f\"Baseline Step 1 完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "threshold=1.0 vs Baseline logits diff: 0.000000\n",
            "结果: ✓ PASS\n"
          ]
        }
      ],
      "source": [
        "# Step 1: TokenSkip threshold=1.0（不应该 skip）\n",
        "tracer.clear()\n",
        "with torch.no_grad():\n",
        "    out1_skip_no = model(\n",
        "        x_skip[:, s:e],\n",
        "        past_key_values=out0_skip.past_key_values,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_position.clone(),\n",
        "        output_hidden_states=True,\n",
        "        skip_layer_k=SKIP_LAYER_K,\n",
        "        skip_threshold=1.0,\n",
        "        skip_outlier=SKIP_OUTLIER,\n",
        "        prev_hidden=prev_hidden\n",
        "    )\n",
        "traces_skip_no_1 = dict(tracer.traces)\n",
        "\n",
        "diff1_no = (out1_baseline.logits - out1_skip_no.logits).abs().max().item()\n",
        "print(f\"threshold=1.0 vs Baseline logits diff: {diff1_no:.6f}\")\n",
        "print(f\"结果: {'✓ PASS' if diff1_no == 0 else '✗ FAIL'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 如果 threshold=1.0 有 diff，逐层检查\n",
        "if diff1_no > 0:\n",
        "    print(\"\\nthreshold=1.0 逐层对比:\")\n",
        "    for i in range(32):\n",
        "        key = f'block_{i}'\n",
        "        if key in traces_baseline_1 and key in traces_skip_no_1:\n",
        "            b_out = traces_baseline_1[key]['output']\n",
        "            s_out = traces_skip_no_1[key]['output']\n",
        "            if b_out.shape == s_out.shape:\n",
        "                diff = (b_out - s_out).abs().max().item()\n",
        "                if diff > 0:\n",
        "                    print(f\"  Layer {i}: diff={diff:.6f}\")\n",
        "            else:\n",
        "                print(f\"  Layer {i}: SHAPE MISMATCH {b_out.shape} vs {s_out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "threshold=0.95 vs Baseline logits diff: 24.875000\n"
          ]
        }
      ],
      "source": [
        "# Step 1: TokenSkip threshold=0.95（可能 skip）\n",
        "tracer.clear()\n",
        "with torch.no_grad():\n",
        "    out1_skip_yes = model(\n",
        "        x_skip[:, s:e],\n",
        "        past_key_values=out0_skip.past_key_values,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_position.clone(),\n",
        "        output_hidden_states=True,\n",
        "        skip_layer_k=SKIP_LAYER_K,\n",
        "        skip_threshold=SKIP_THRESHOLD,\n",
        "        skip_outlier=SKIP_OUTLIER,\n",
        "        prev_hidden=prev_hidden\n",
        "    )\n",
        "traces_skip_yes_1 = dict(tracer.traces)\n",
        "\n",
        "diff1_yes = (out1_baseline.logits - out1_skip_yes.logits).abs().max().item()\n",
        "print(f\"threshold={SKIP_THRESHOLD} vs Baseline logits diff: {diff1_yes:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 逐层形状对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "逐层输入形状对比（Baseline vs TokenSkip threshold=0.95）:\n",
            "Layer 0-7: Loop 1, Layer 8-31: Loop 2\n",
            "--------------------------------------------------------------------------------\n",
            "Layer  0: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  1: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  2: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  3: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  4: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  5: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  6: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  7: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096])\n",
            "Layer  8: baseline=torch.Size([1, 32, 4096]), skip=N/A <-- split *** DIFF ***\n",
            "Layer  9: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 10: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 11: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 12: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 13: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 14: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 15: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 16: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 17: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 18: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 19: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 20: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 21: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 22: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 23: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 24: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 25: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 26: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 27: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 28: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 29: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 30: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n",
            "Layer 31: baseline=torch.Size([1, 32, 4096]), skip=N/A *** DIFF ***\n"
          ]
        }
      ],
      "source": [
        "print(\"逐层输入形状对比（Baseline vs TokenSkip threshold=0.95）:\")\n",
        "print(f\"Layer 0-{SKIP_LAYER_K-1}: Loop 1, Layer {SKIP_LAYER_K}-31: Loop 2\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i in range(32):\n",
        "    key = f'block_{i}'\n",
        "    b = traces_baseline_1.get(key, {})\n",
        "    sk = traces_skip_yes_1.get(key, {})\n",
        "    \n",
        "    b_in = b.get('input_shape', 'N/A')\n",
        "    sk_in = sk.get('input_shape', 'N/A')\n",
        "    \n",
        "    marker = \" <-- split\" if i == SKIP_LAYER_K else \"\"\n",
        "    if b_in != sk_in:\n",
        "        marker += \" *** DIFF ***\"\n",
        "    \n",
        "    print(f\"Layer {i:2d}: baseline={b_in}, skip={sk_in}{marker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q/K/V 投影对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q/K/V 投影对比（关键层）:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Layer 0:\n",
            "  q_proj: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "  k_proj: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "  v_proj: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "\n",
            "Layer 7:\n",
            "  q_proj: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "  k_proj: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "  v_proj: baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "\n",
            "Layer 8:\n",
            "  q_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "  k_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "  v_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "\n",
            "Layer 9:\n",
            "  q_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "  k_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "  v_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "\n",
            "Layer 31:\n",
            "  q_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "  k_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "  v_proj: baseline=torch.Size([1, 32, 4096]), skip=N/A\n"
          ]
        }
      ],
      "source": [
        "print(\"Q/K/V 投影对比（关键层）:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i in [0, SKIP_LAYER_K-1, SKIP_LAYER_K, SKIP_LAYER_K+1, 31]:\n",
        "    print(f\"\\nLayer {i}:\")\n",
        "    for proj in ['q_proj', 'k_proj', 'v_proj']:\n",
        "        key = f'block_{i}_{proj}'\n",
        "        b = traces_baseline_1.get(key, {})\n",
        "        sk = traces_skip_yes_1.get(key, {})\n",
        "        \n",
        "        b_out = b.get('output_shape', 'N/A')\n",
        "        sk_out = sk.get('output_shape', 'N/A')\n",
        "        \n",
        "        diff_str = \"\"\n",
        "        if 'output' in b and 'output' in sk:\n",
        "            bo, so = b['output'], sk['output']\n",
        "            if bo.shape == so.shape:\n",
        "                diff_str = f\", diff={( bo - so).abs().max().item():.6f}\"\n",
        "        \n",
        "        print(f\"  {proj}: baseline={b_out}, skip={sk_out}{diff_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RoPE 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoPE 对比（关键层）:\n",
            "--------------------------------------------------------------------------------\n",
            "Layer 0: baseline_out=torch.Size([1, 32, 32, 128]), skip_out=torch.Size([1, 32, 32, 128])\n",
            "Layer 7: baseline_out=torch.Size([1, 32, 32, 128]), skip_out=torch.Size([1, 32, 32, 128])\n",
            "Layer 8: baseline_out=torch.Size([1, 32, 32, 128]), skip_out=N/A\n",
            "Layer 9: baseline_out=torch.Size([1, 32, 32, 128]), skip_out=N/A\n",
            "Layer 31: baseline_out=torch.Size([1, 32, 32, 128]), skip_out=N/A\n"
          ]
        }
      ],
      "source": [
        "print(\"RoPE 对比（关键层）:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i in [0, SKIP_LAYER_K-1, SKIP_LAYER_K, SKIP_LAYER_K+1, 31]:\n",
        "    key = f'block_{i}_rope'\n",
        "    b = traces_baseline_1.get(key, {})\n",
        "    sk = traces_skip_yes_1.get(key, {})\n",
        "    \n",
        "    # RoPE 输出是 (q, k) tuple\n",
        "    b_out = b.get('output_shape', 'N/A')\n",
        "    sk_out = sk.get('output_shape', 'N/A')\n",
        "    \n",
        "    print(f\"Layer {i}: baseline_out={b_out}, skip_out={sk_out}\")\n",
        "    \n",
        "    # 详细看 q, k 形状\n",
        "    if 'output' in b:\n",
        "        bo = b['output']\n",
        "        if isinstance(bo, tuple):\n",
        "            print(f\"  baseline q={bo[0].shape if hasattr(bo[0], 'shape') else 'N/A'}, k={bo[1].shape if len(bo)>1 and hasattr(bo[1], 'shape') else 'N/A'}\")\n",
        "    if 'output' in sk:\n",
        "        so = sk['output']\n",
        "        if isinstance(so, tuple):\n",
        "            print(f\"  skip     q={so[0].shape if hasattr(so[0], 'shape') else 'N/A'}, k={so[1].shape if len(so)>1 and hasattr(so[1], 'shape') else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention 输出对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention 输出对比（关键层）:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Layer 0:\n",
            "  baseline: out=torch.Size([1, 32, 4096])\n",
            "  skip:     out=torch.Size([1, 32, 4096])\n",
            "  diff: 0.000000\n",
            "\n",
            "Layer 7:\n",
            "  baseline: out=torch.Size([1, 32, 4096])\n",
            "  skip:     out=torch.Size([1, 32, 4096])\n",
            "  diff: 0.000000\n",
            "\n",
            "Layer 8:\n",
            "  baseline: out=torch.Size([1, 32, 4096])\n",
            "  skip:     out=None\n",
            "\n",
            "Layer 9:\n",
            "  baseline: out=torch.Size([1, 32, 4096])\n",
            "  skip:     out=None\n",
            "\n",
            "Layer 31:\n",
            "  baseline: out=torch.Size([1, 32, 4096])\n",
            "  skip:     out=None\n"
          ]
        }
      ],
      "source": [
        "print(\"Attention 输出对比（关键层）:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i in [0, SKIP_LAYER_K-1, SKIP_LAYER_K, SKIP_LAYER_K+1, 31]:\n",
        "    key = f'block_{i}_attn_out'\n",
        "    b = traces_baseline_1.get(key, {})\n",
        "    sk = traces_skip_yes_1.get(key, {})\n",
        "    \n",
        "    print(f\"\\nLayer {i}:\")\n",
        "    print(f\"  baseline: out={b.get('output_shape')}\")\n",
        "    print(f\"  skip:     out={sk.get('output_shape')}\")\n",
        "    \n",
        "    if 'output' in b and 'output' in sk:\n",
        "        bo, so = b['output'], sk['output']\n",
        "        if bo.shape == so.shape:\n",
        "            print(f\"  diff: {(bo - so).abs().max().item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FFN 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFN 对比（关键层）:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Layer 0:\n",
            "  ff_proj : baseline=torch.Size([1, 32, 12288]), skip=torch.Size([1, 32, 12288]), diff=0.000000\n",
            "  up_proj : baseline=torch.Size([1, 32, 12288]), skip=torch.Size([1, 32, 12288]), diff=0.000000\n",
            "  ffn_out : baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "\n",
            "Layer 7:\n",
            "  ff_proj : baseline=torch.Size([1, 32, 12288]), skip=torch.Size([1, 32, 12288]), diff=0.000000\n",
            "  up_proj : baseline=torch.Size([1, 32, 12288]), skip=torch.Size([1, 32, 12288]), diff=0.000000\n",
            "  ffn_out : baseline=torch.Size([1, 32, 4096]), skip=torch.Size([1, 32, 4096]), diff=0.000000\n",
            "\n",
            "Layer 8:\n",
            "  ff_proj : baseline=torch.Size([1, 32, 12288]), skip=N/A\n",
            "  up_proj : baseline=torch.Size([1, 32, 12288]), skip=N/A\n",
            "  ffn_out : baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "\n",
            "Layer 9:\n",
            "  ff_proj : baseline=torch.Size([1, 32, 12288]), skip=N/A\n",
            "  up_proj : baseline=torch.Size([1, 32, 12288]), skip=N/A\n",
            "  ffn_out : baseline=torch.Size([1, 32, 4096]), skip=N/A\n",
            "\n",
            "Layer 31:\n",
            "  ff_proj : baseline=torch.Size([1, 32, 12288]), skip=N/A\n",
            "  up_proj : baseline=torch.Size([1, 32, 12288]), skip=N/A\n",
            "  ffn_out : baseline=torch.Size([1, 32, 4096]), skip=N/A\n"
          ]
        }
      ],
      "source": [
        "print(\"FFN 对比（关键层）:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i in [0, SKIP_LAYER_K-1, SKIP_LAYER_K, SKIP_LAYER_K+1, 31]:\n",
        "    print(f\"\\nLayer {i}:\")\n",
        "    for proj in ['ff_proj', 'up_proj', 'ffn_out']:\n",
        "        key = f'block_{i}_{proj}'\n",
        "        b = traces_baseline_1.get(key, {})\n",
        "        sk = traces_skip_yes_1.get(key, {})\n",
        "        \n",
        "        b_out = b.get('output_shape', 'N/A')\n",
        "        sk_out = sk.get('output_shape', 'N/A')\n",
        "        \n",
        "        diff_str = \"\"\n",
        "        if 'output' in b and 'output' in sk:\n",
        "            bo, so = b['output'], sk['output']\n",
        "            if bo.shape == so.shape:\n",
        "                diff_str = f\", diff={( bo - so).abs().max().item():.6f}\"\n",
        "        \n",
        "        print(f\"  {proj:8s}: baseline={b_out}, skip={sk_out}{diff_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 手动计算 cos_sim 判定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cos_sim 统计（每个 token 的 layer 1-7）:\n",
            "  token min cos: 0.992188 ~ 1.000000\n",
            "  token mean cos: 0.997210 ~ 1.000000\n"
          ]
        }
      ],
      "source": [
        "L = BLOCK_LENGTH\n",
        "all_cos_sims = []\n",
        "\n",
        "for j in range(L):\n",
        "    cos_sims_j = []\n",
        "    for layer in range(1, SKIP_LAYER_K):\n",
        "        h1 = prev_hidden[layer][0, j, :]\n",
        "        h2 = out1_skip_no.hidden_states[layer][0, j, :]\n",
        "        cos = F.cosine_similarity(h1.unsqueeze(0), h2.unsqueeze(0), dim=-1).item()\n",
        "        cos = min(1.0, cos)\n",
        "        cos_sims_j.append(cos)\n",
        "    all_cos_sims.append(cos_sims_j)\n",
        "\n",
        "all_min = [min(c) for c in all_cos_sims]\n",
        "all_mean = [sum(c)/len(c) for c in all_cos_sims]\n",
        "\n",
        "print(f\"cos_sim 统计（每个 token 的 layer 1-{SKIP_LAYER_K-1}）:\")\n",
        "print(f\"  token min cos: {min(all_min):.6f} ~ {max(all_min):.6f}\")\n",
        "print(f\"  token mean cos: {min(all_mean):.6f} ~ {max(all_mean):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "判定结果:\n",
            "  Active: 0, Stable: 32\n",
            "  Active: []\n",
            "  Stable: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]...\n"
          ]
        }
      ],
      "source": [
        "# 判定\n",
        "active_mask = []\n",
        "for j in range(L):\n",
        "    min_cos = min(all_cos_sims[j])\n",
        "    mean_cos = sum(all_cos_sims[j]) / len(all_cos_sims[j])\n",
        "    stable = min_cos >= SKIP_OUTLIER and mean_cos > SKIP_THRESHOLD\n",
        "    active_mask.append(not stable)\n",
        "\n",
        "num_active = sum(active_mask)\n",
        "num_stable = L - num_active\n",
        "active_indices = [i for i, a in enumerate(active_mask) if a]\n",
        "stable_indices = [i for i, a in enumerate(active_mask) if not a]\n",
        "\n",
        "print(f\"判定结果:\")\n",
        "print(f\"  Active: {num_active}, Stable: {num_stable}\")\n",
        "print(f\"  Active: {active_indices[:15]}{'...' if len(active_indices) > 15 else ''}\")\n",
        "print(f\"  Stable: {stable_indices[:15]}{'...' if len(stable_indices) > 15 else ''}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "每个 token 的 cos_sim:\n",
            "  Token  0: min=0.996094, mean=0.998326 -> stable\n",
            "  Token  1: min=0.992188, mean=0.998326 -> stable\n",
            "  Token  2: min=0.992188, mean=0.998326 -> stable\n",
            "  Token  3: min=0.996094, mean=0.999442 -> stable\n",
            "  Token  4: min=0.992188, mean=0.998326 -> stable\n",
            "  Token  5: min=0.996094, mean=0.998884 -> stable\n",
            "  Token  6: min=0.992188, mean=0.997210 -> stable\n",
            "  Token  7: min=1.000000, mean=1.000000 -> stable\n",
            "  Token  8: min=0.992188, mean=0.997210 -> stable\n",
            "  Token  9: min=0.992188, mean=0.997210 -> stable\n",
            "  Token 10: min=0.992188, mean=0.998326 -> stable\n",
            "  Token 11: min=1.000000, mean=1.000000 -> stable\n",
            "  Token 12: min=1.000000, mean=1.000000 -> stable\n",
            "  Token 13: min=0.996094, mean=0.998884 -> stable\n",
            "  Token 14: min=0.992188, mean=0.997768 -> stable\n",
            "  Token 15: min=0.996094, mean=0.998884 -> stable\n",
            "  Token 16: min=0.992188, mean=0.997210 -> stable\n",
            "  Token 17: min=0.996094, mean=0.998326 -> stable\n",
            "  Token 18: min=0.996094, mean=0.999442 -> stable\n",
            "  Token 19: min=0.992188, mean=0.998326 -> stable\n",
            "  Token 20: min=0.992188, mean=0.997768 -> stable\n",
            "  Token 21: min=0.992188, mean=0.998326 -> stable\n",
            "  Token 22: min=0.996094, mean=0.999442 -> stable\n",
            "  Token 23: min=0.992188, mean=0.998884 -> stable\n",
            "  Token 24: min=0.992188, mean=0.997768 -> stable\n",
            "  Token 25: min=0.992188, mean=0.998884 -> stable\n",
            "  Token 26: min=0.992188, mean=0.997210 -> stable\n",
            "  Token 27: min=0.996094, mean=0.998884 -> stable\n",
            "  Token 28: min=0.996094, mean=0.998884 -> stable\n",
            "  Token 29: min=0.992188, mean=0.997768 -> stable\n",
            "  Token 30: min=0.992188, mean=0.997768 -> stable\n",
            "  Token 31: min=0.992188, mean=0.997210 -> stable\n"
          ]
        }
      ],
      "source": [
        "# 每个 token 详细\n",
        "print(\"每个 token 的 cos_sim:\")\n",
        "for j in range(L):\n",
        "    min_cos = min(all_cos_sims[j])\n",
        "    mean_cos = sum(all_cos_sims[j]) / len(all_cos_sims[j])\n",
        "    status = \"stable\" if not active_mask[j] else \"active\"\n",
        "    print(f\"  Token {j:2d}: min={min_cos:.6f}, mean={mean_cos:.6f} -> {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KV Cache 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KV Cache 对比（关键层）:\n",
            "--------------------------------------------------------------------------------\n",
            "Layer  0: K_diff=0.000000, V_diff=0.000000\n",
            "Layer  7: K_diff=0.000000, V_diff=0.000000\n",
            "Layer  8: K_diff=0.000000, V_diff=0.000000\n",
            "Layer  9: K_diff=0.000000, V_diff=0.000000\n",
            "Layer 31: K_diff=0.000000, V_diff=0.000000\n"
          ]
        }
      ],
      "source": [
        "print(\"KV Cache 对比（关键层）:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "kv_base = out1_baseline.past_key_values\n",
        "kv_skip = out1_skip_yes.past_key_values\n",
        "\n",
        "for i in [0, SKIP_LAYER_K-1, SKIP_LAYER_K, SKIP_LAYER_K+1, 31]:\n",
        "    k_base, v_base = kv_base[i]\n",
        "    k_skip, v_skip = kv_skip[i]\n",
        "    \n",
        "    k_diff = (k_base - k_skip).abs().max().item()\n",
        "    v_diff = (v_base - v_skip).abs().max().item()\n",
        "    \n",
        "    print(f\"Layer {i:2d}: K_diff={k_diff:.6f}, V_diff={v_diff:.6f}\")\n",
        "    \n",
        "    if k_diff > 0:\n",
        "        diff_per_pos = (k_base - k_skip).abs().max(dim=1)[0].max(dim=-1)[0][0]\n",
        "        diff_positions = (diff_per_pos > 0).nonzero(as_tuple=True)[0].tolist()\n",
        "        print(f\"         K diff positions: {diff_positions[:10]}{'...' if len(diff_positions) > 10 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hidden States 逐 token 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "最后一层 hidden states:\n",
            "  Baseline: torch.Size([32, 4096])\n",
            "  Skip: torch.Size([32, 4096])\n",
            "\n",
            "逐 token diff:\n",
            "  Token  0 (stable): diff=179.000000 ***\n",
            "  Token  1 (stable): diff=156.000000 ***\n",
            "  Token  2 (stable): diff=164.000000 ***\n",
            "  Token  3 (stable): diff=185.000000 ***\n",
            "  Token  4 (stable): diff=190.000000 ***\n",
            "  Token  5 (stable): diff=189.000000 ***\n",
            "  Token  6 (stable): diff=191.000000 ***\n",
            "  Token  7 (stable): diff=181.000000 ***\n",
            "  Token  8 (stable): diff=184.000000 ***\n",
            "  Token  9 (stable): diff=182.000000 ***\n",
            "  Token 10 (stable): diff=184.000000 ***\n",
            "  Token 11 (stable): diff=180.000000 ***\n",
            "  Token 12 (stable): diff=166.000000 ***\n",
            "  Token 13 (stable): diff=183.000000 ***\n",
            "  Token 14 (stable): diff=179.000000 ***\n",
            "  Token 15 (stable): diff=180.000000 ***\n",
            "  Token 16 (stable): diff=172.000000 ***\n",
            "  Token 17 (stable): diff=172.000000 ***\n",
            "  Token 18 (stable): diff=178.000000 ***\n",
            "  Token 19 (stable): diff=171.000000 ***\n",
            "  Token 20 (stable): diff=172.000000 ***\n",
            "  Token 21 (stable): diff=173.000000 ***\n",
            "  Token 22 (stable): diff=177.000000 ***\n",
            "  Token 23 (stable): diff=184.000000 ***\n",
            "  Token 24 (stable): diff=174.000000 ***\n",
            "  Token 25 (stable): diff=180.000000 ***\n",
            "  Token 26 (stable): diff=176.000000 ***\n",
            "  Token 27 (stable): diff=180.000000 ***\n",
            "  Token 28 (stable): diff=176.000000 ***\n",
            "  Token 29 (stable): diff=174.000000 ***\n",
            "  Token 30 (stable): diff=170.000000 ***\n",
            "  Token 31 (stable): diff=169.000000 ***\n"
          ]
        }
      ],
      "source": [
        "h_base = out1_baseline.hidden_states[-1][0]\n",
        "h_skip = out1_skip_yes.hidden_states[-1][0]\n",
        "\n",
        "print(f\"最后一层 hidden states:\")\n",
        "print(f\"  Baseline: {h_base.shape}\")\n",
        "print(f\"  Skip: {h_skip.shape}\")\n",
        "\n",
        "if h_base.shape == h_skip.shape:\n",
        "    print(\"\\n逐 token diff:\")\n",
        "    for j in range(min(L, h_base.shape[0])):\n",
        "        diff = (h_base[j] - h_skip[j]).abs().max().item()\n",
        "        status = \"stable\" if not active_mask[j] else \"active\"\n",
        "        marker = \" ***\" if diff > 0.01 else \"\"\n",
        "        print(f\"  Token {j:2d} ({status:6s}): diff={diff:.6f}{marker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logits 逐 token 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits 逐 token 对比:\n",
            "  Token  0 (stable): diff=19.250000, pred=✗\n",
            "  Token  1 (stable): diff=17.375000, pred=✗\n",
            "  Token  2 (stable): diff=19.000000, pred=✗\n",
            "  Token  3 (stable): diff=20.750000, pred=✗\n",
            "  Token  4 (stable): diff=20.250000, pred=✗\n",
            "  Token  5 (stable): diff=22.875000, pred=✗\n",
            "  Token  6 (stable): diff=22.500000, pred=✗\n",
            "  Token  7 (stable): diff=22.125000, pred=✗\n",
            "  Token  8 (stable): diff=23.000000, pred=✗\n",
            "  Token  9 (stable): diff=20.625000, pred=✗\n",
            "  Token 10 (stable): diff=18.750000, pred=✗\n",
            "  Token 11 (stable): diff=19.750000, pred=✗\n",
            "  Token 12 (stable): diff=20.875000, pred=✗\n",
            "  Token 13 (stable): diff=20.000000, pred=✗\n",
            "  Token 14 (stable): diff=21.000000, pred=✗\n",
            "  Token 15 (stable): diff=23.500000, pred=✗\n",
            "  Token 16 (stable): diff=19.875000, pred=✗\n",
            "  Token 17 (stable): diff=20.750000, pred=✗\n",
            "  Token 18 (stable): diff=22.375000, pred=✗\n",
            "  Token 19 (stable): diff=20.375000, pred=✗\n",
            "  Token 20 (stable): diff=20.750000, pred=✗\n",
            "  Token 21 (stable): diff=20.875000, pred=✗\n",
            "  Token 22 (stable): diff=21.750000, pred=✗\n",
            "  Token 23 (stable): diff=24.875000, pred=✗\n",
            "  Token 24 (stable): diff=20.750000, pred=✗\n",
            "  Token 25 (stable): diff=23.750000, pred=✗\n",
            "  Token 26 (stable): diff=22.250000, pred=✗\n",
            "  Token 27 (stable): diff=24.125000, pred=✗\n",
            "  Token 28 (stable): diff=20.250000, pred=✗\n",
            "  Token 29 (stable): diff=19.750000, pred=✗\n",
            "  Token 30 (stable): diff=19.500000, pred=✗\n",
            "  Token 31 (stable): diff=21.000000, pred=✗\n"
          ]
        }
      ],
      "source": [
        "logits_base = out1_baseline.logits[0]\n",
        "logits_skip = out1_skip_yes.logits[0]\n",
        "\n",
        "print(\"Logits 逐 token 对比:\")\n",
        "for j in range(min(L, logits_base.shape[0])):\n",
        "    diff = (logits_base[j] - logits_skip[j]).abs().max().item()\n",
        "    status = \"stable\" if not active_mask[j] else \"active\"\n",
        "    \n",
        "    pred_base = logits_base[j].argmax().item()\n",
        "    pred_skip = logits_skip[j].argmax().item()\n",
        "    pred_match = \"✓\" if pred_base == pred_skip else \"✗\"\n",
        "    \n",
        "    print(f\"  Token {j:2d} ({status:6s}): diff={diff:.6f}, pred={pred_match}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 清理 & 总结"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "总结\n",
            "============================================================\n",
            "\n",
            "Step 0 (prev_hidden=None):\n",
            "  logits diff: 0.000000 ✓\n",
            "\n",
            "Step 1 (threshold=1.0):\n",
            "  logits diff: 0.000000 ✓\n",
            "\n",
            "Step 1 (threshold=0.95):\n",
            "  Active: 0, Stable: 32\n",
            "  logits diff: 24.875000\n",
            "\n",
            "关键检查:\n",
            "  1. Step 0 diff = 0 ✓\n",
            "  2. threshold=1.0 diff = 0 -> 退化正确\n",
            "  3. threshold<1 有 skip -> 正常有差异\n"
          ]
        }
      ],
      "source": [
        "tracer.remove()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"总结\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nStep 0 (prev_hidden=None):\")\n",
        "print(f\"  logits diff: {diff0_logits:.6f} {'✓' if diff0_logits == 0 else '✗'}\")\n",
        "print(f\"\\nStep 1 (threshold=1.0):\")\n",
        "print(f\"  logits diff: {diff1_no:.6f} {'✓' if diff1_no == 0 else '✗'}\")\n",
        "print(f\"\\nStep 1 (threshold={SKIP_THRESHOLD}):\")\n",
        "print(f\"  Active: {num_active}, Stable: {num_stable}\")\n",
        "print(f\"  logits diff: {diff1_yes:.6f}\")\n",
        "print(\"\\n关键检查:\")\n",
        "print(\"  1. Step 0 diff = 0 ✓\")\n",
        "print(\"  2. threshold=1.0 diff = 0 -> 退化正确\")\n",
        "print(\"  3. threshold<1 有 skip -> 正常有差异\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 修复诊断单元（替换上面报错/警告）\n",
        "如果上面的单元报 NameError 或 warning，请改运行下面这两格。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m out_base, nfe_base \u001b[38;5;241m=\u001b[39m generate_with_dual_cache(\n\u001b[1;32m     15\u001b[0m     model,\n\u001b[1;32m     16\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m out_skip, nfe_skip \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_dual_cache_tokenskip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGEN_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layer_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSKIP_LAYER_K\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSKIP_THRESHOLD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_outlier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSKIP_OUTLIER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m base_gen \u001b[38;5;241m=\u001b[39m out_base[\u001b[38;5;241m0\u001b[39m, prompt_len:]\n\u001b[1;32m     36\u001b[0m skip_gen \u001b[38;5;241m=\u001b[39m out_skip[\u001b[38;5;241m0\u001b[39m, prompt_len:]\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/generate.py:305\u001b[0m, in \u001b[0;36mgenerate_with_dual_cache_tokenskip\u001b[0;34m(model, prompt, steps, gen_length, block_length, temperature, remasking, mask_id, threshold, factor, skip_layer_k, skip_threshold, skip_outlier)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# 模型前向（Token Skip 逻辑在 model 内部双 loop 实现）\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m out_blk \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layer_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_layer_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_outlier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_outlier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m logits_blk \u001b[38;5;241m=\u001b[39m out_blk\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    317\u001b[0m prev_hidden \u001b[38;5;241m=\u001b[39m out_blk\u001b[38;5;241m.\u001b[39mhidden_states  \u001b[38;5;66;03m# 保存供下一 step 判定\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:1708\u001b[0m, in \u001b[0;36mLLaDAModelLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, replace_position, position_ids, skip_layer_k, skip_threshold, skip_outlier, prev_hidden)\u001b[0m\n\u001b[1;32m   1705\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1708\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layer_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_layer_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_outlier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_outlier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:1591\u001b[0m, in \u001b[0;36mLLaDAModel.forward\u001b[0;34m(self, input_ids, input_embeddings, attention_mask, attention_bias, past_key_values, use_cache, last_logits_only, output_hidden_states, replace_position, position_ids, skip_layer_k, skip_threshold, skip_outlier, prev_hidden)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     x, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\n\u001b[1;32m   1588\u001b[0m         block, x, attention_bias\u001b[38;5;241m=\u001b[39mattention_bias, layer_past\u001b[38;5;241m=\u001b[39mlayer_past, use_cache\u001b[38;5;241m=\u001b[39muse_cache,replace_position\u001b[38;5;241m=\u001b[39mreplace_position,position_ids\u001b[38;5;241m=\u001b[39mposition_ids\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     x, cache \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:981\u001b[0m, in \u001b[0;36mLLaDALlamaBlock.forward\u001b[0;34m(self, x, attention_bias, layer_past, use_cache, replace_position, position_ids)\u001b[0m\n\u001b[1;32m    977\u001b[0m     att, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention, q, k, v, attention_bias, layer_past\u001b[38;5;241m=\u001b[39mlayer_past, use_cache\u001b[38;5;241m=\u001b[39muse_cache,replace_position\u001b[38;5;241m=\u001b[39mreplace_position,position_ids\u001b[38;5;241m=\u001b[39mposition_ids\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 981\u001b[0m     att, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# Add attention scores.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# shape: (B, T, C)\u001b[39;00m\n\u001b[1;32m    985\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(att)\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:766\u001b[0m, in \u001b[0;36mLLaDABlock.attention\u001b[0;34m(self, q, k, v, mask, attention_bias, layer_past, use_cache, replace_position, position_ids)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# Apply rotary embeddings.\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;66;03m# 使用传入的位置（支持不连续）\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m         q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m replace_position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m         q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(q, k)\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:459\u001b[0m, in \u001b[0;36mRotaryEmbedding.forward\u001b[0;34m(self, q, k, block_end_index, position_ids)\u001b[0m\n\u001b[1;32m    457\u001b[0m     pos_cos_slice \u001b[38;5;241m=\u001b[39m pos_cos\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m2\u001b[39m, idx)\n\u001b[1;32m    458\u001b[0m     q_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_rotary_pos_emb(pos_sin_slice, pos_cos_slice, q_)\n\u001b[0;32m--> 459\u001b[0m     k_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_sin_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_cos_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Baseline 情况：q 是当前 block，k 是完整 KV cache\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m block_end_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:433\u001b[0m, in \u001b[0;36mRotaryEmbedding.apply_rotary_pos_emb\u001b[0;34m(self, pos_sin, pos_cos, t)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_rotary_pos_emb\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos_sin: torch\u001b[38;5;241m.\u001b[39mTensor, pos_cos: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ((\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_cos\u001b[49m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate_half(t) \u001b[38;5;241m*\u001b[39m pos_sin))\u001b[38;5;241m.\u001b[39mto(t\u001b[38;5;241m.\u001b[39mdtype)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (147) must match the size of tensor b (2) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "from generate import generate_with_dual_cache, generate_with_dual_cache_tokenskip\n",
        "\n",
        "# 重新跑：Baseline vs TokenSkip 的 mask 残留与长度\n",
        "MASK_ID = safe_get(\"MASK_ID\", 126336)\n",
        "GEN_LENGTH = safe_get(\"GEN_LENGTH\", 128)\n",
        "BLOCK_LENGTH = safe_get(\"BLOCK_LENGTH\", 32)\n",
        "STEPS = safe_get(\"STEPS\", 128)\n",
        "\n",
        "if \"input_ids\" not in globals():\n",
        "    raise RuntimeError(\"input_ids 未定义，请先运行上面的输入准备单元\")\n",
        "\n",
        "prompt_len = int(input_ids.shape[1])\n",
        "\n",
        "out_base, nfe_base = generate_with_dual_cache(\n",
        "    model,\n",
        "    input_ids,\n",
        "    steps=STEPS,\n",
        "    gen_length=GEN_LENGTH,\n",
        "    block_length=BLOCK_LENGTH,\n",
        "    threshold=0.9,\n",
        ")\n",
        "\n",
        "out_skip, nfe_skip = generate_with_dual_cache_tokenskip(\n",
        "    model,\n",
        "    input_ids,\n",
        "    steps=STEPS,\n",
        "    gen_length=GEN_LENGTH,\n",
        "    block_length=BLOCK_LENGTH,\n",
        "    threshold=0.9,\n",
        "    skip_layer_k=safe_get(\"SKIP_LAYER_K\", 8),\n",
        "    skip_threshold=safe_get(\"SKIP_THRESHOLD\", 0.95),\n",
        "    skip_outlier=safe_get(\"SKIP_OUTLIER\", 0.7),\n",
        ")\n",
        "\n",
        "base_gen = out_base[0, prompt_len:]\n",
        "skip_gen = out_skip[0, prompt_len:]\n",
        "\n",
        "base_mask_res = count_mask_residual(base_gen, MASK_ID)\n",
        "skip_mask_res = count_mask_residual(skip_gen, MASK_ID)\n",
        "\n",
        "base_tokens_len = int((base_gen != MASK_ID).sum().item())\n",
        "skip_tokens_len = int((skip_gen != MASK_ID).sum().item())\n",
        "\n",
        "base_blocks = block_mask_stats(out_base[0], prompt_len, GEN_LENGTH, BLOCK_LENGTH, MASK_ID)\n",
        "skip_blocks = block_mask_stats(out_skip[0], prompt_len, GEN_LENGTH, BLOCK_LENGTH, MASK_ID)\n",
        "\n",
        "logging.info(\"Baseline: nfe=%d, mask_residual=%d, gen_tokens=%d\", nfe_base, base_mask_res, base_tokens_len)\n",
        "logging.info(\"TokenSkip: nfe=%d, mask_residual=%d, gen_tokens=%d\", nfe_skip, skip_mask_res, skip_tokens_len)\n",
        "logging.info(\"Baseline block mask residuals: %s\", base_blocks)\n",
        "logging.info(\"TokenSkip block mask residuals: %s\", skip_blocks)\n",
        "\n",
        "if skip_mask_res > 0:\n",
        "    logging.warning(\"TokenSkip 存在 mask 残留，可能导致 decode 变短\")\n",
        "\n",
        "# 修复 warning：active_mask 兼容 tensor/list\n",
        "if \"x_skip\" in globals() and \"s\" in globals() and \"e\" in globals() and \"active_mask\" in globals():\n",
        "    mask_blk = (x_skip[:, s:e] == MASK_ID)[0]\n",
        "    if isinstance(active_mask, torch.Tensor):\n",
        "        stable_mask = ~active_mask\n",
        "    else:\n",
        "        stable_mask = ~torch.tensor(active_mask, device=x_skip.device)\n",
        "\n",
        "    if stable_mask.all() and mask_blk.any():\n",
        "        logging.error(\"检测到：全部稳定但仍有 mask，后续层可能被完全跳过\")\n",
        "    else:\n",
        "        logging.info(\"稳定/Mask 检查完成，未触发危险状态\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 诊断：Mask 残留与稳定判定\n",
        "用于定位 tokenskip 导致生成长度变短的问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "def setup_logger(level=logging.INFO):\n",
        "    logging.basicConfig(\n",
        "        level=level,\n",
        "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        "    )\n",
        "\n",
        "\n",
        "def count_mask_residual(tokens: torch.Tensor, mask_id: int) -> int:\n",
        "    return int((tokens == mask_id).sum().item())\n",
        "\n",
        "\n",
        "def block_mask_stats(tokens: torch.Tensor, prompt_len: int, gen_length: int, block_length: int, mask_id: int):\n",
        "    stats = []\n",
        "    num_blocks = gen_length // block_length\n",
        "    for nb in range(num_blocks):\n",
        "        s = prompt_len + nb * block_length\n",
        "        e = s + block_length\n",
        "        block = tokens[s:e]\n",
        "        stats.append(count_mask_residual(block, mask_id))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def safe_get(name, default=None):\n",
        "    return globals().get(name, default)\n",
        "\n",
        "\n",
        "setup_logger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m out_base, nfe_base \u001b[38;5;241m=\u001b[39m generate_with_dual_cache(\n\u001b[1;32m     13\u001b[0m     model,\n\u001b[1;32m     14\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m out_skip, nfe_skip \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_dual_cache_tokenskip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGEN_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layer_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSKIP_LAYER_K\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSKIP_THRESHOLD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_outlier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSKIP_OUTLIER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m base_gen \u001b[38;5;241m=\u001b[39m out_base[\u001b[38;5;241m0\u001b[39m, prompt_len:]\n\u001b[1;32m     34\u001b[0m skip_gen \u001b[38;5;241m=\u001b[39m out_skip[\u001b[38;5;241m0\u001b[39m, prompt_len:]\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/generate.py:305\u001b[0m, in \u001b[0;36mgenerate_with_dual_cache_tokenskip\u001b[0;34m(model, prompt, steps, gen_length, block_length, temperature, remasking, mask_id, threshold, factor, skip_layer_k, skip_threshold, skip_outlier)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# 模型前向（Token Skip 逻辑在 model 内部双 loop 实现）\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m out_blk \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layer_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_layer_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_outlier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_outlier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m logits_blk \u001b[38;5;241m=\u001b[39m out_blk\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    317\u001b[0m prev_hidden \u001b[38;5;241m=\u001b[39m out_blk\u001b[38;5;241m.\u001b[39mhidden_states  \u001b[38;5;66;03m# 保存供下一 step 判定\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:1708\u001b[0m, in \u001b[0;36mLLaDAModelLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, replace_position, position_ids, skip_layer_k, skip_threshold, skip_outlier, prev_hidden)\u001b[0m\n\u001b[1;32m   1705\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1708\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layer_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_layer_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_outlier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_outlier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:1591\u001b[0m, in \u001b[0;36mLLaDAModel.forward\u001b[0;34m(self, input_ids, input_embeddings, attention_mask, attention_bias, past_key_values, use_cache, last_logits_only, output_hidden_states, replace_position, position_ids, skip_layer_k, skip_threshold, skip_outlier, prev_hidden)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     x, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\n\u001b[1;32m   1588\u001b[0m         block, x, attention_bias\u001b[38;5;241m=\u001b[39mattention_bias, layer_past\u001b[38;5;241m=\u001b[39mlayer_past, use_cache\u001b[38;5;241m=\u001b[39muse_cache,replace_position\u001b[38;5;241m=\u001b[39mreplace_position,position_ids\u001b[38;5;241m=\u001b[39mposition_ids\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     x, cache \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:981\u001b[0m, in \u001b[0;36mLLaDALlamaBlock.forward\u001b[0;34m(self, x, attention_bias, layer_past, use_cache, replace_position, position_ids)\u001b[0m\n\u001b[1;32m    977\u001b[0m     att, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention, q, k, v, attention_bias, layer_past\u001b[38;5;241m=\u001b[39mlayer_past, use_cache\u001b[38;5;241m=\u001b[39muse_cache,replace_position\u001b[38;5;241m=\u001b[39mreplace_position,position_ids\u001b[38;5;241m=\u001b[39mposition_ids\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 981\u001b[0m     att, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreplace_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# Add attention scores.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# shape: (B, T, C)\u001b[39;00m\n\u001b[1;32m    985\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(att)\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:766\u001b[0m, in \u001b[0;36mLLaDABlock.attention\u001b[0;34m(self, q, k, v, mask, attention_bias, layer_past, use_cache, replace_position, position_ids)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# Apply rotary embeddings.\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;66;03m# 使用传入的位置（支持不连续）\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m         q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m replace_position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m         q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(q, k)\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:459\u001b[0m, in \u001b[0;36mRotaryEmbedding.forward\u001b[0;34m(self, q, k, block_end_index, position_ids)\u001b[0m\n\u001b[1;32m    457\u001b[0m     pos_cos_slice \u001b[38;5;241m=\u001b[39m pos_cos\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m2\u001b[39m, idx)\n\u001b[1;32m    458\u001b[0m     q_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_rotary_pos_emb(pos_sin_slice, pos_cos_slice, q_)\n\u001b[0;32m--> 459\u001b[0m     k_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_sin_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_cos_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Baseline 情况：q 是当前 block，k 是完整 KV cache\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m block_end_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/mnt/d/fastllmskip/FastdLLM/llada/model/modeling_llada.py:433\u001b[0m, in \u001b[0;36mRotaryEmbedding.apply_rotary_pos_emb\u001b[0;34m(self, pos_sin, pos_cos, t)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_rotary_pos_emb\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos_sin: torch\u001b[38;5;241m.\u001b[39mTensor, pos_cos: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ((\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_cos\u001b[49m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate_half(t) \u001b[38;5;241m*\u001b[39m pos_sin))\u001b[38;5;241m.\u001b[39mto(t\u001b[38;5;241m.\u001b[39mdtype)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (147) must match the size of tensor b (2) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "# 对比 Baseline vs TokenSkip 的 mask 残留与长度\n",
        "MASK_ID = safe_get(\"MASK_ID\", 126336)\n",
        "GEN_LENGTH = safe_get(\"GEN_LENGTH\", 128)\n",
        "BLOCK_LENGTH = safe_get(\"BLOCK_LENGTH\", 32)\n",
        "STEPS = safe_get(\"STEPS\", 128)\n",
        "\n",
        "if \"input_ids\" not in globals():\n",
        "    raise RuntimeError(\"input_ids 未定义，请先运行上面的输入准备单元\")\n",
        "\n",
        "prompt_len = int(input_ids.shape[1])\n",
        "\n",
        "out_base, nfe_base = generate_with_dual_cache(\n",
        "    model,\n",
        "    input_ids,\n",
        "    steps=STEPS,\n",
        "    gen_length=GEN_LENGTH,\n",
        "    block_length=BLOCK_LENGTH,\n",
        "    threshold=0.9,\n",
        ")\n",
        "\n",
        "out_skip, nfe_skip = generate_with_dual_cache_tokenskip(\n",
        "    model,\n",
        "    input_ids,\n",
        "    steps=STEPS,\n",
        "    gen_length=GEN_LENGTH,\n",
        "    block_length=BLOCK_LENGTH,\n",
        "    threshold=0.9,\n",
        "    skip_layer_k=safe_get(\"SKIP_LAYER_K\", 8),\n",
        "    skip_threshold=safe_get(\"SKIP_THRESHOLD\", 0.95),\n",
        "    skip_outlier=safe_get(\"SKIP_OUTLIER\", 0.7),\n",
        ")\n",
        "\n",
        "base_gen = out_base[0, prompt_len:]\n",
        "skip_gen = out_skip[0, prompt_len:]\n",
        "\n",
        "base_mask_res = count_mask_residual(base_gen, MASK_ID)\n",
        "skip_mask_res = count_mask_residual(skip_gen, MASK_ID)\n",
        "\n",
        "base_tokens_len = int((base_gen != MASK_ID).sum().item())\n",
        "skip_tokens_len = int((skip_gen != MASK_ID).sum().item())\n",
        "\n",
        "base_blocks = block_mask_stats(out_base[0], prompt_len, GEN_LENGTH, BLOCK_LENGTH, MASK_ID)\n",
        "skip_blocks = block_mask_stats(out_skip[0], prompt_len, GEN_LENGTH, BLOCK_LENGTH, MASK_ID)\n",
        "\n",
        "logging.info(\"Baseline: nfe=%d, mask_residual=%d, gen_tokens=%d\", nfe_base, base_mask_res, base_tokens_len)\n",
        "logging.info(\"TokenSkip: nfe=%d, mask_residual=%d, gen_tokens=%d\", nfe_skip, skip_mask_res, skip_tokens_len)\n",
        "logging.info(\"Baseline block mask residuals: %s\", base_blocks)\n",
        "logging.info(\"TokenSkip block mask residuals: %s\", skip_blocks)\n",
        "\n",
        "if skip_mask_res > 0:\n",
        "    logging.warning(\"TokenSkip 存在 mask 残留，可能导致 decode 变短\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-23 13:41:59,730 INFO mask positions in block: 0\n",
            "2026-01-23 13:41:59,731 INFO stable positions in block: 32\n",
            "2026-01-23 13:41:59,732 INFO stable AND mask positions: 0\n"
          ]
        }
      ],
      "source": [
        "# 检查：稳定判定是否包含 mask 位置\n",
        "if \"prev_hidden\" not in globals() or \"out1_skip_no\" not in globals():\n",
        "    logging.warning(\"prev_hidden 或 out1_skip_no 未定义，请先运行 Step 0/1 的单元\")\n",
        "else:\n",
        "    skip_layer_k = safe_get(\"SKIP_LAYER_K\", 8)\n",
        "    skip_threshold = safe_get(\"SKIP_THRESHOLD\", 0.95)\n",
        "    skip_outlier = safe_get(\"SKIP_OUTLIER\", 0.7)\n",
        "\n",
        "    cur_hidden = out1_skip_no.hidden_states  # 当前 step 的 hidden\n",
        "    L = cur_hidden[0].shape[1]\n",
        "\n",
        "    # 复现模型内的稳定判定\n",
        "    active_mask = []\n",
        "    for j in range(L):\n",
        "        cos_sims = []\n",
        "        for layer in range(1, min(skip_layer_k, len(cur_hidden))):\n",
        "            h1 = prev_hidden[layer][0, j, :]\n",
        "            h2 = cur_hidden[layer][0, j, :]\n",
        "            cos = F.cosine_similarity(h1.unsqueeze(0), h2.unsqueeze(0), dim=-1).item()\n",
        "            cos = min(1.0, cos)\n",
        "            cos_sims.append(cos)\n",
        "        stable = len(cos_sims) > 0 and min(cos_sims) >= skip_outlier and sum(cos_sims) / len(cos_sims) > skip_threshold\n",
        "        active_mask.append(not stable)\n",
        "\n",
        "    active_mask = torch.tensor(active_mask, device=cur_hidden[0].device)\n",
        "    stable_mask = ~active_mask\n",
        "\n",
        "    if \"x_skip\" not in globals() or \"s\" not in globals() or \"e\" not in globals():\n",
        "        logging.warning(\"x_skip 或 s/e 未定义，无法检查 mask 交集\")\n",
        "    else:\n",
        "        mask_blk = (x_skip[:, s:e] == MASK_ID)[0]\n",
        "        stable_and_mask = stable_mask & mask_blk\n",
        "        logging.info(\"mask positions in block: %d\", int(mask_blk.sum().item()))\n",
        "        logging.info(\"stable positions in block: %d\", int(stable_mask.sum().item()))\n",
        "        logging.info(\"stable AND mask positions: %d\", int(stable_and_mask.sum().item()))\n",
        "        if stable_and_mask.any():\n",
        "            idx = stable_and_mask.nonzero(as_tuple=True)[0].tolist()\n",
        "            logging.warning(\"存在 mask 被判定为稳定: indices=%s\", idx[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 检查：是否出现“全部稳定但仍有 mask”的危险状态\n",
        "if \"x_skip\" in globals() and \"s\" in globals() and \"e\" in globals():\n",
        "    mask_blk = (x_skip[:, s:e] == MASK_ID)[0]\n",
        "    if \"active_mask\" in globals():\n",
        "        try:\n",
        "            stable_mask = ~torch.tensor(active_mask, device=x_skip.device)\n",
        "        except Exception:\n",
        "            stable_mask = None\n",
        "    else:\n",
        "        stable_mask = None\n",
        "\n",
        "    if stable_mask is not None:\n",
        "        if stable_mask.all() and mask_blk.any():\n",
        "            logging.error(\"检测到：全部稳定但仍有 mask，后续层可能被完全跳过\")\n",
        "    else:\n",
        "        logging.warning(\"active_mask 未定义，跳过该检查\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 诊断补充：逐 step 追踪 mask / stable 交集\n",
        "定位“mask 被判稳定导致后半层跳过”的具体 step。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-23 13:42:05,153 INFO Block 0 Step 1: prev_hidden=None (不判定稳定)\n",
            "2026-01-23 13:42:05,364 INFO Block 0 Step 2: mask=30, stable=30, stable&mask=29\n",
            "2026-01-23 13:42:05,365 WARNING Block 0 Step 2: mask 被判稳定，indices=[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
            "2026-01-23 13:42:05,499 INFO Block 0 Step 3: mask=29, stable=30, stable&mask=29\n",
            "2026-01-23 13:42:05,500 WARNING Block 0 Step 3: mask 被判稳定，indices=[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
            "2026-01-23 13:42:05,659 INFO Block 0 Step 4: mask=28, stable=31, stable&mask=28\n",
            "2026-01-23 13:42:05,660 WARNING Block 0 Step 4: mask 被判稳定，indices=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
            "2026-01-23 13:42:05,811 INFO Block 0 Step 5: mask=27, stable=30, stable&mask=26\n",
            "2026-01-23 13:42:05,812 WARNING Block 0 Step 5: mask 被判稳定，indices=[4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
            "2026-01-23 13:42:05,954 INFO Block 0 Step 6: mask=26, stable=31, stable&mask=26\n",
            "2026-01-23 13:42:05,955 WARNING Block 0 Step 6: mask 被判稳定，indices=[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
            "2026-01-23 13:42:06,110 INFO Block 0 Step 7: mask=25, stable=31, stable&mask=25\n",
            "2026-01-23 13:42:06,111 WARNING Block 0 Step 7: mask 被判稳定，indices=[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
            "2026-01-23 13:42:06,115 INFO Block 0: step-wise 追踪结束\n"
          ]
        }
      ],
      "source": [
        "# 逐 step 追踪：mask 数量 / stable 数量 / stable∩mask\n",
        "# 只跑前 DEBUG_BLOCKS 个 block、每个 block 前 DEBUG_STEPS 个 step，避免太重\n",
        "\n",
        "if \"x_init\" not in globals():\n",
        "    raise RuntimeError(\"x_init 未定义，请先运行上面的输入准备单元\")\n",
        "\n",
        "MASK_ID = safe_get(\"MASK_ID\", 126336)\n",
        "GEN_LENGTH = safe_get(\"GEN_LENGTH\", 128)\n",
        "BLOCK_LENGTH = safe_get(\"BLOCK_LENGTH\", 32)\n",
        "STEPS = safe_get(\"STEPS\", 128)\n",
        "SKIP_LAYER_K = safe_get(\"SKIP_LAYER_K\", 8)\n",
        "SKIP_THRESHOLD = safe_get(\"SKIP_THRESHOLD\", 0.95)\n",
        "SKIP_OUTLIER = safe_get(\"SKIP_OUTLIER\", 0.7)\n",
        "\n",
        "x = x_init.clone()\n",
        "num_blocks = GEN_LENGTH // BLOCK_LENGTH\n",
        "steps_per_block = STEPS // num_blocks\n",
        "\n",
        "max_stable_mask = 0\n",
        "first_hit = None\n",
        "\n",
        "for nb in range(min(num_blocks, DEBUG_BLOCKS)):\n",
        "    s = prompt_len + nb * BLOCK_LENGTH\n",
        "    e = s + BLOCK_LENGTH\n",
        "\n",
        "    # 当前 block 的 mask 数\n",
        "    block_mask_index = (x[:, s:e] == MASK_ID)\n",
        "    num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n",
        "\n",
        "    # Step 0: 全序列 forward\n",
        "    out_full = model(x, use_cache=True)\n",
        "    past_key_values = out_full.past_key_values\n",
        "\n",
        "    replace_position = torch.zeros_like(x, dtype=torch.bool)\n",
        "    replace_position[:, s:e] = True\n",
        "\n",
        "    global_mask_index = (x == MASK_ID)\n",
        "    global_mask_index[:, e:] = False\n",
        "\n",
        "    quota0 = None  # 使用 threshold 模式\n",
        "    x0, transfer_index = get_transfer_index(\n",
        "        out_full.logits,\n",
        "        temperature=0.0,\n",
        "        remasking=\"low_confidence\",\n",
        "        mask_index=global_mask_index,\n",
        "        x=x,\n",
        "        num_transfer_tokens=quota0,\n",
        "        threshold=0.9,\n",
        "    )\n",
        "    x = torch.where(transfer_index, x0, x)\n",
        "\n",
        "    prev_hidden = None\n",
        "\n",
        "    for i in range(1, min(steps_per_block, DEBUG_STEPS)):\n",
        "        mask_blk = (x[:, s:e] == MASK_ID)\n",
        "        mask_cnt = int(mask_blk.sum().item())\n",
        "        if mask_cnt == 0:\n",
        "            logging.info(\"Block %d Step %d: mask=0, break\", nb, i)\n",
        "            break\n",
        "\n",
        "        # 为了避免触发模型内部 skip（可能导致 position_ids 形状不匹配），\n",
        "        # 这里强制关闭 skip，仅用于生成隐藏状态并在外部判定稳定。\n",
        "        out_blk = model(\n",
        "            x[:, s:e],\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=True,\n",
        "            replace_position=replace_position,\n",
        "            output_hidden_states=True,\n",
        "            skip_layer_k=None,\n",
        "            prev_hidden=None,\n",
        "        )\n",
        "\n",
        "        # 复现稳定判定（和模型逻辑一致）\n",
        "        if prev_hidden is not None:\n",
        "            cur_hidden = out_blk.hidden_states\n",
        "            L = cur_hidden[0].shape[1]\n",
        "            stable_mask = torch.ones(L, dtype=torch.bool, device=cur_hidden[0].device)\n",
        "            for j in range(L):\n",
        "                cos_sims = []\n",
        "                for layer in range(1, min(SKIP_LAYER_K, len(cur_hidden))):\n",
        "                    h1 = prev_hidden[layer][0, j, :]\n",
        "                    h2 = cur_hidden[layer][0, j, :]\n",
        "                    cos = F.cosine_similarity(h1.unsqueeze(0), h2.unsqueeze(0), dim=-1).item()\n",
        "                    cos = min(1.0, cos)\n",
        "                    cos_sims.append(cos)\n",
        "                if len(cos_sims) > 0 and min(cos_sims) >= SKIP_OUTLIER and sum(cos_sims) / len(cos_sims) > SKIP_THRESHOLD:\n",
        "                    # 稳定\n",
        "                    continue\n",
        "                stable_mask[j] = False\n",
        "\n",
        "            stable_cnt = int(stable_mask.sum().item())\n",
        "            stable_and_mask = stable_mask & mask_blk[0]\n",
        "            stable_and_mask_cnt = int(stable_and_mask.sum().item())\n",
        "            logging.info(\n",
        "                \"Block %d Step %d: mask=%d, stable=%d, stable&mask=%d\",\n",
        "                nb, i, mask_cnt, stable_cnt, stable_and_mask_cnt\n",
        "            )\n",
        "            if stable_and_mask_cnt > 0:\n",
        "                idx = stable_and_mask.nonzero(as_tuple=True)[0].tolist()\n",
        "                logging.warning(\"Block %d Step %d: mask 被判稳定，indices=%s\", nb, i, idx[:20])\n",
        "                max_stable_mask = max(max_stable_mask, stable_and_mask_cnt)\n",
        "                if first_hit is None:\n",
        "                    first_hit = (nb, i, stable_and_mask_cnt)\n",
        "        else:\n",
        "            logging.info(\"Block %d Step %d: prev_hidden=None (不判定稳定)\", nb, i)\n",
        "\n",
        "        logits_blk = out_blk.logits\n",
        "        x0_blk, transfer_idx_blk = get_transfer_index(\n",
        "            logits_blk,\n",
        "            temperature=0.0,\n",
        "            remasking=\"low_confidence\",\n",
        "            mask_index=mask_blk,\n",
        "            x=x[:, s:e],\n",
        "            num_transfer_tokens=None,\n",
        "            threshold=0.9,\n",
        "        )\n",
        "\n",
        "        blk_old = x[:, s:e]\n",
        "        blk_new = torch.where(transfer_idx_blk, x0_blk, blk_old)\n",
        "        x = torch.cat([x[:, :s], blk_new, x[:, e:]], dim=1)\n",
        "        prev_hidden = out_blk.hidden_states\n",
        "\n",
        "    logging.info(\"Block %d: step-wise 追踪结束\", nb)\n",
        "\n",
        "if first_hit is not None:\n",
        "    nb, i, cnt = first_hit\n",
        "    logging.error(\"结论: 发现 mask 被判稳定（首个命中: block=%d, step=%d, stable&mask=%d）\", nb, i, cnt)\n",
        "    logging.error(\"推断: skip 判定未排除 mask，导致后半层跳过 -> 生成长度变短\")\n",
        "else:\n",
        "    logging.info(\"结论: 未发现 mask 被判稳定（本次 DEBUG 范围内）\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
