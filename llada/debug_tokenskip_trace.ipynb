{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TokenSkip 分歧追踪 Notebook\n",
        "\n",
        "**目标**: 对比 `skip_threshold=1` 和 `skip_threshold=0.9999` 的差异\n",
        "\n",
        "**方法**: 从输出结果往回追溯，逐 step 对比，找出第一个分歧点\n",
        "\n",
        "**关键问题**: 为什么 threshold=0.9999 时输出有重复/质量下降？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pianng/miniconda3/envs/dllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型加载完成，32 层\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "from model.modeling_llada import LLaDAModelLM\n",
        "from generate import get_transfer_index, get_num_transfer_tokens\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "device = 'cuda'\n",
        "model = LLaDAModelLM.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', torch_dtype=torch.bfloat16).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct')\n",
        "print(f\"模型加载完成，{len(model.model.transformer.blocks)} 层\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 证明：prev_hidden 和 current_hidden 维度不匹配\n",
        "\n",
        "直接模拟 `generate_with_dual_cache_tokenskip` 的行为，打印每一步的 hidden states 形状。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt_len: 19\n",
            "total_len: 147\n",
            "x.shape: torch.Size([1, 147])\n",
            "\n",
            "Block 0 范围: [19, 51)\n",
            "\n",
            "============================================================\n",
            "Step 0: 完整前向（模拟 generate_with_dual_cache_tokenskip）\n",
            "============================================================\n",
            "模型输入 x.shape: torch.Size([1, 147])\n",
            "prev_hidden 层数: 33\n",
            "prev_hidden[0].shape: torch.Size([1, 147, 4096])  <-- 完整序列长度 147\n",
            "\n",
            "采样后 x 中 block 0 的 mask 数: 0\n",
            "\n",
            "============================================================\n",
            "Step 1: 只输入 block 部分\n",
            "============================================================\n",
            "模型输入 x[:, s:e].shape: torch.Size([1, 32])  <-- 只有 block 长度 32\n",
            "current_hidden 层数: 33\n",
            "current_hidden[0].shape: torch.Size([1, 32, 4096])  <-- 只有 block 长度 32\n",
            "\n",
            "============================================================\n",
            "关键对比：prev_hidden vs current_hidden\n",
            "============================================================\n",
            "\n",
            "prev_hidden[0].shape:    torch.Size([1, 147, 4096])\n",
            "current_hidden[0].shape: torch.Size([1, 32, 4096])\n",
            "\n",
            ">>> 维度不匹配！147 ≠ 32 <<<\n",
            "\n",
            "============================================================\n",
            "在 Token Skip 判定中会发生什么？\n",
            "============================================================\n",
            "\n",
            "Token Skip 判定代码（modeling_llada.py line 1517-1524）：\n",
            "  for j in range(L):  # L = x.shape[1] = 32\n",
            "      h1 = prev_hidden[layer][0, j, :]   # j=0 → 序列位置 0\n",
            "      h2 = all_hidden_states[layer][0, j, :]  # j=0 → 序列位置 19\n",
            "\n",
            "具体例子：\n",
            "  当 j=0 时:\n",
            "    h1 = prev_hidden[1][0, 0, :]  → 序列位置 0 (prompt 第一个 token)\n",
            "    h2 = current_hidden[1][0, 0, :] → 序列位置 19 (block 第一个 token)\n",
            "\n",
            "  这两个 token 完全不同！比较它们的 cos_sim 没有意义！\n",
            "\n",
            "错误的 cos_sim（位置 0 vs 位置 19）: 0.717535\n",
            "正确的 cos_sim（位置 19 vs 位置 19）: 0.820543\n",
            "\n",
            ">>> 结论：当前代码比较的是完全不同的 token！<<<\n"
          ]
        }
      ],
      "source": [
        "# 证明 prev_hidden 和 current_hidden 维度不匹配\n",
        "# 这里直接模拟 generate_with_dual_cache_tokenskip 的行为\n",
        "\n",
        "MASK_ID = 126336\n",
        "prompt = \"Who is Newton, physics?\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "input_ids = torch.tensor(tokenizer(text)['input_ids']).to(device).unsqueeze(0)\n",
        "\n",
        "GEN_LENGTH = 128\n",
        "BLOCK_LENGTH = 32\n",
        "prompt_len = input_ids.shape[1]\n",
        "total_len = prompt_len + GEN_LENGTH\n",
        "\n",
        "# 初始化 x\n",
        "x = torch.full((1, total_len), MASK_ID, dtype=torch.long, device=device)\n",
        "x[:, :prompt_len] = input_ids\n",
        "\n",
        "print(f\"prompt_len: {prompt_len}\")\n",
        "print(f\"total_len: {total_len}\")\n",
        "print(f\"x.shape: {x.shape}\")\n",
        "print()\n",
        "\n",
        "# Block 0 的范围\n",
        "s = prompt_len  # 19\n",
        "e = s + BLOCK_LENGTH  # 51\n",
        "print(f\"Block 0 范围: [{s}, {e})\")\n",
        "print()\n",
        "\n",
        "# ============ Step 0: 完整前向（和 generate_with_dual_cache_tokenskip 一样）============\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 0: 完整前向（模拟 generate_with_dual_cache_tokenskip）\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_full = model(x, use_cache=True, output_hidden_states=True)\n",
        "\n",
        "past_kv = out_full.past_key_values\n",
        "prev_hidden = out_full.hidden_states\n",
        "\n",
        "print(f\"模型输入 x.shape: {x.shape}\")\n",
        "print(f\"prev_hidden 层数: {len(prev_hidden)}\")\n",
        "print(f\"prev_hidden[0].shape: {prev_hidden[0].shape}  <-- 完整序列长度 {total_len}\")\n",
        "print()\n",
        "\n",
        "# 做初始采样（简化，只更新当前 block）\n",
        "replace_position = torch.zeros_like(x, dtype=torch.bool)\n",
        "replace_position[:, s:e] = True\n",
        "\n",
        "global_mask = (x == MASK_ID)\n",
        "global_mask[:, e:] = False\n",
        "x0 = out_full.logits.argmax(dim=-1)\n",
        "# 简单处理：更新所有 mask 位置\n",
        "x = torch.where(global_mask, x0, x)\n",
        "\n",
        "print(f\"采样后 x 中 block 0 的 mask 数: {(x[:, s:e] == MASK_ID).sum().item()}\")\n",
        "print()\n",
        "\n",
        "# ============ Step 1: 只输入 block（和 generate_with_dual_cache_tokenskip 一样）============\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 1: 只输入 block 部分\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_blk = model(\n",
        "        x[:, s:e],  # 只输入 block 部分\n",
        "        past_key_values=past_kv,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_position,\n",
        "        output_hidden_states=True,\n",
        "        skip_layer_k=18,\n",
        "        skip_threshold=1.0,  # 先用 1.0 不触发 skip\n",
        "        skip_outlier=0.7,\n",
        "        prev_hidden=prev_hidden,  # 传入完整序列的 hidden states\n",
        "    )\n",
        "\n",
        "current_hidden = out_blk.hidden_states\n",
        "\n",
        "print(f\"模型输入 x[:, s:e].shape: {x[:, s:e].shape}  <-- 只有 block 长度 {BLOCK_LENGTH}\")\n",
        "print(f\"current_hidden 层数: {len(current_hidden)}\")\n",
        "print(f\"current_hidden[0].shape: {current_hidden[0].shape}  <-- 只有 block 长度 {BLOCK_LENGTH}\")\n",
        "print()\n",
        "\n",
        "# ============ 关键对比 ============\n",
        "print(\"=\" * 60)\n",
        "print(\"关键对比：prev_hidden vs current_hidden\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(f\"prev_hidden[0].shape:    {prev_hidden[0].shape}\")\n",
        "print(f\"current_hidden[0].shape: {current_hidden[0].shape}\")\n",
        "print()\n",
        "print(f\">>> 维度不匹配！{prev_hidden[0].shape[1]} ≠ {current_hidden[0].shape[1]} <<<\")\n",
        "print()\n",
        "\n",
        "# ============ 这会导致什么问题？============\n",
        "print(\"=\" * 60)\n",
        "print(\"在 Token Skip 判定中会发生什么？\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"Token Skip 判定代码（modeling_llada.py line 1517-1524）：\")\n",
        "print(\"  for j in range(L):  # L = x.shape[1] = 32\")\n",
        "print(\"      h1 = prev_hidden[layer][0, j, :]   # j=0 → 序列位置 0\")\n",
        "print(\"      h2 = all_hidden_states[layer][0, j, :]  # j=0 → 序列位置 19\")\n",
        "print()\n",
        "print(\"具体例子：\")\n",
        "print(f\"  当 j=0 时:\")\n",
        "print(f\"    h1 = prev_hidden[1][0, 0, :]  → 序列位置 0 (prompt 第一个 token)\")\n",
        "print(f\"    h2 = current_hidden[1][0, 0, :] → 序列位置 {s} (block 第一个 token)\")\n",
        "print()\n",
        "print(f\"  这两个 token 完全不同！比较它们的 cos_sim 没有意义！\")\n",
        "print()\n",
        "\n",
        "# 实际计算一下 cos_sim\n",
        "h1 = prev_hidden[1][0, 0, :]  # 位置 0\n",
        "h2 = current_hidden[1][0, 0, :]  # 位置 s (19)\n",
        "cos_wrong = F.cosine_similarity(h1.unsqueeze(0).float(), h2.unsqueeze(0).float(), dim=-1).item()\n",
        "\n",
        "# 如果我们用正确的位置呢？\n",
        "h1_correct = prev_hidden[1][0, s, :]  # 位置 s (19)\n",
        "h2_correct = current_hidden[1][0, 0, :]  # 也是位置 s (19)\n",
        "cos_correct = F.cosine_similarity(h1_correct.unsqueeze(0).float(), h2_correct.unsqueeze(0).float(), dim=-1).item()\n",
        "\n",
        "print(f\"错误的 cos_sim（位置 0 vs 位置 {s}）: {cos_wrong:.6f}\")\n",
        "print(f\"正确的 cos_sim（位置 {s} vs 位置 {s}）: {cos_correct:.6f}\")\n",
        "print()\n",
        "print(\">>> 结论：当前代码比较的是完全不同的 token！<<<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt_len: 19, total_len: 147\n",
            "num_blocks: 4, steps_per_block: 8\n"
          ]
        }
      ],
      "source": [
        "# 准备输入\n",
        "prompt = \"Who is Newton, physics?\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "input_ids = torch.tensor(tokenizer(text)['input_ids']).to(device).unsqueeze(0)\n",
        "\n",
        "MASK_ID = 126336\n",
        "GEN_LENGTH = 128\n",
        "BLOCK_LENGTH = 32\n",
        "STEPS = 32\n",
        "THRESHOLD = 0.9  # 置信度阈值\n",
        "\n",
        "# Token Skip 超参\n",
        "SKIP_LAYER_K = 18\n",
        "SKIP_OUTLIER = 0.7\n",
        "\n",
        "prompt_len = input_ids.shape[1]\n",
        "total_len = prompt_len + GEN_LENGTH\n",
        "num_blocks = GEN_LENGTH // BLOCK_LENGTH\n",
        "steps_per_block = STEPS // num_blocks\n",
        "\n",
        "print(f\"prompt_len: {prompt_len}, total_len: {total_len}\")\n",
        "print(f\"num_blocks: {num_blocks}, steps_per_block: {steps_per_block}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 完整运行两个版本，记录每一步的状态"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_with_trace(model, prompt, skip_threshold, skip_layer_k=18, skip_outlier=0.7):\n",
        "    \"\"\"\n",
        "    运行 generate 并记录每一步的详细状态\n",
        "    返回: (final_x, trace_log)\n",
        "    trace_log 包含每个 block 的每个 step 的详细信息\n",
        "    \"\"\"\n",
        "    B = prompt.shape[0]\n",
        "    Lp = int(prompt.shape[1])\n",
        "    \n",
        "    x = torch.full((B, Lp + GEN_LENGTH), MASK_ID, dtype=torch.long, device=model.device)\n",
        "    x[:, :Lp] = prompt\n",
        "    \n",
        "    trace_log = defaultdict(list)  # {block_idx: [step_info, ...]}\n",
        "    \n",
        "    for nb in range(num_blocks):\n",
        "        s = Lp + nb * BLOCK_LENGTH\n",
        "        e = s + BLOCK_LENGTH\n",
        "        \n",
        "        block_mask_index = (x[:, s:e] == MASK_ID)\n",
        "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n",
        "        \n",
        "        # Step 0: 完整前向\n",
        "        out_full = model(x, use_cache=True, output_hidden_states=True)\n",
        "        past_key_values = out_full.past_key_values\n",
        "        \n",
        "        replace_position = torch.zeros_like(x, dtype=torch.bool)\n",
        "        replace_position[:, s:e] = True\n",
        "        \n",
        "        global_mask_index = (x == MASK_ID)\n",
        "        global_mask_index[:, e:] = False\n",
        "        \n",
        "        x0, transfer_index = get_transfer_index(\n",
        "            out_full.logits, 0., \"low_confidence\", global_mask_index, x, None, THRESHOLD\n",
        "        )\n",
        "        \n",
        "        # 记录 Step 0\n",
        "        step_info = {\n",
        "            'step': 0,\n",
        "            'x_before': x[:, s:e].clone(),\n",
        "            'x_after': torch.where(transfer_index, x0, x)[:, s:e].clone(),\n",
        "            'transfer_count': transfer_index[:, s:e].sum().item(),\n",
        "            'mask_count_before': (x[:, s:e] == MASK_ID).sum().item(),\n",
        "            'logits_sample': out_full.logits[:, s:e, :100].clone(),  # 只记录前 100 个 vocab\n",
        "            'prev_hidden': None,\n",
        "            'active_mask': None,\n",
        "            'cos_sims': None,\n",
        "        }\n",
        "        trace_log[nb].append(step_info)\n",
        "        \n",
        "        x = torch.where(transfer_index, x0, x)\n",
        "        prev_hidden = out_full.hidden_states\n",
        "        \n",
        "        # Step 1 ~ N\n",
        "        for i in range(1, steps_per_block):\n",
        "            mask_count = (x[:, s:e] == MASK_ID).sum().item()\n",
        "            if mask_count == 0:\n",
        "                break\n",
        "            \n",
        "            # 调用模型（带 Token Skip）\n",
        "            out_blk = model(\n",
        "                x[:, s:e],\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                replace_position=replace_position,\n",
        "                output_hidden_states=True,\n",
        "                skip_layer_k=skip_layer_k,\n",
        "                skip_threshold=skip_threshold,\n",
        "                skip_outlier=skip_outlier,\n",
        "                prev_hidden=prev_hidden,\n",
        "            )\n",
        "            logits_blk = out_blk.logits\n",
        "            \n",
        "            # === 手动计算 cos_sim 以记录（和模型内部一样的逻辑）===\n",
        "            cos_sims_all = []\n",
        "            active_mask_manual = torch.ones(BLOCK_LENGTH, dtype=torch.bool, device=x.device)\n",
        "            \n",
        "            if prev_hidden is not None:\n",
        "                current_hidden = out_blk.hidden_states\n",
        "                for j in range(BLOCK_LENGTH):\n",
        "                    cos_sims_j = []\n",
        "                    for layer in range(1, min(skip_layer_k, len(current_hidden))):\n",
        "                        # 注意: prev_hidden 和 current_hidden 的形状可能不同！\n",
        "                        if prev_hidden[layer].shape[1] > j and current_hidden[layer].shape[1] > j:\n",
        "                            h1 = prev_hidden[layer][0, j, :]\n",
        "                            h2 = current_hidden[layer][0, j, :]\n",
        "                            cos = F.cosine_similarity(h1.unsqueeze(0), h2.unsqueeze(0), dim=-1).item()\n",
        "                            cos = min(1.0, cos)\n",
        "                            cos_sims_j.append(cos)\n",
        "                    \n",
        "                    cos_sims_all.append(cos_sims_j)\n",
        "                    \n",
        "                    if len(cos_sims_j) > 0:\n",
        "                        avg_cos = sum(cos_sims_j) / len(cos_sims_j)\n",
        "                        min_cos = min(cos_sims_j)\n",
        "                        if min_cos >= skip_outlier and avg_cos > skip_threshold:\n",
        "                            active_mask_manual[j] = False  # 稳定，可跳过\n",
        "            \n",
        "            # 获取 transfer index\n",
        "            mask_blk = (x[:, s:e] == MASK_ID)\n",
        "            x0_blk, transfer_idx_blk = get_transfer_index(\n",
        "                logits_blk, 0., \"low_confidence\", mask_blk, x[:, s:e], None, THRESHOLD\n",
        "            )\n",
        "            \n",
        "            # 记录这一步\n",
        "            step_info = {\n",
        "                'step': i,\n",
        "                'x_before': x[:, s:e].clone(),\n",
        "                'x_after': None,  # 稍后填充\n",
        "                'transfer_count': transfer_idx_blk.sum().item(),\n",
        "                'mask_count_before': mask_count,\n",
        "                'logits_sample': logits_blk[:, :, :100].clone(),\n",
        "                'prev_hidden_shape': [h.shape for h in prev_hidden] if prev_hidden else None,\n",
        "                'current_hidden_shape': [h.shape for h in out_blk.hidden_states],\n",
        "                'active_mask': active_mask_manual.clone(),\n",
        "                'skip_count': (~active_mask_manual).sum().item(),\n",
        "                'cos_sims': cos_sims_all,  # 每个 token 的 cos_sim 列表\n",
        "            }\n",
        "            \n",
        "            # 更新 x\n",
        "            blk_old = x[:, s:e]\n",
        "            blk_new = torch.where(transfer_idx_blk, x0_blk, blk_old)\n",
        "            x = torch.cat([x[:, :s], blk_new, x[:, e:]], dim=1)\n",
        "            \n",
        "            step_info['x_after'] = x[:, s:e].clone()\n",
        "            trace_log[nb].append(step_info)\n",
        "            \n",
        "            prev_hidden = out_blk.hidden_states\n",
        "    \n",
        "    return x, trace_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "运行 threshold=1 版本...\n",
            "threshold=1 输出:\n",
            "Isaac Newton was an English physicist and mathematician who\n",
            "\n",
            "输出长度: 128 tokens\n"
          ]
        }
      ],
      "source": [
        "# 运行 threshold=1 版本\n",
        "print(\"运行 threshold=1 版本...\")\n",
        "with torch.no_grad():\n",
        "    x_t1, trace_t1 = run_with_trace(model, input_ids, skip_threshold=1.0)\n",
        "\n",
        "ans_t1 = tokenizer.decode(x_t1[0, prompt_len:], skip_special_tokens=True)\n",
        "print(f\"threshold=1 输出:\")\n",
        "print(ans_t1)\n",
        "print(f\"\\n输出长度: {len(x_t1[0, prompt_len:])} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "运行 threshold=0.9999 版本...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "# 运行 threshold=0.9999 版本\n",
        "print(\"运行 threshold=0.9999 版本...\")\n",
        "with torch.no_grad():\n",
        "    x_t2, trace_t2 = run_with_trace(model, input_ids, skip_threshold=0.99)\n",
        "\n",
        "ans_t2 = tokenizer.decode(x_t2[0, prompt_len:], skip_special_tokens=True)\n",
        "print(f\"threshold=0.9999 输出:\")\n",
        "print(ans_t2)\n",
        "print(f\"\\n输出长度: {len(x_t2[0, prompt_len:])} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 对比两个版本的最终输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Token 级别对比\n",
            "============================================================\n",
            "\n",
            "总共 0 个位置不同\n"
          ]
        }
      ],
      "source": [
        "# Token 级别对比\n",
        "print(\"=\" * 60)\n",
        "print(\"Token 级别对比\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tokens_t1 = x_t1[0, prompt_len:].tolist()\n",
        "tokens_t2 = x_t2[0, prompt_len:].tolist()\n",
        "\n",
        "diff_positions = []\n",
        "for i, (t1, t2) in enumerate(zip(tokens_t1, tokens_t2)):\n",
        "    if t1 != t2:\n",
        "        diff_positions.append(i)\n",
        "        w1 = tokenizer.decode([t1])\n",
        "        w2 = tokenizer.decode([t2])\n",
        "        print(f\"位置 {i}: threshold=1 -> '{w1}' (id={t1}), threshold=0.9999 -> '{w2}' (id={t2})\")\n",
        "\n",
        "print(f\"\\n总共 {len(diff_positions)} 个位置不同\")\n",
        "if diff_positions:\n",
        "    print(f\"第一个不同的位置: {diff_positions[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 找出第一个分歧点（从 trace 往回查）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "第一个分歧点: block=None, step=None, 原因=no divergence found\n"
          ]
        }
      ],
      "source": [
        "def find_first_divergence(trace_t1, trace_t2):\n",
        "    \"\"\"找出第一个分歧的 (block, step)\"\"\"\n",
        "    for nb in range(num_blocks):\n",
        "        steps_t1 = trace_t1[nb]\n",
        "        steps_t2 = trace_t2[nb]\n",
        "        \n",
        "        max_steps = max(len(steps_t1), len(steps_t2))\n",
        "        \n",
        "        for i in range(max_steps):\n",
        "            if i >= len(steps_t1) or i >= len(steps_t2):\n",
        "                return nb, i, \"step count mismatch\"\n",
        "            \n",
        "            s1 = steps_t1[i]\n",
        "            s2 = steps_t2[i]\n",
        "            \n",
        "            # 比较 x_after\n",
        "            if s1['x_after'] is not None and s2['x_after'] is not None:\n",
        "                if not torch.equal(s1['x_after'], s2['x_after']):\n",
        "                    return nb, i, \"x_after diverged\"\n",
        "            \n",
        "            # 比较 logits\n",
        "            if not torch.allclose(s1['logits_sample'], s2['logits_sample'], atol=1e-3):\n",
        "                return nb, i, \"logits diverged\"\n",
        "    \n",
        "    return None, None, \"no divergence found\"\n",
        "\n",
        "div_block, div_step, div_reason = find_first_divergence(trace_t1, trace_t2)\n",
        "print(f\"第一个分歧点: block={div_block}, step={div_step}, 原因={div_reason}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 详细查看分歧点\n",
        "if div_block is not None and div_step is not None:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"分歧点详情: Block {div_block}, Step {div_step}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    s1 = trace_t1[div_block][div_step]\n",
        "    s2 = trace_t2[div_block][div_step]\n",
        "    \n",
        "    print(f\"\\n--- threshold=1 ---\")\n",
        "    print(f\"mask_count_before: {s1['mask_count_before']}\")\n",
        "    print(f\"transfer_count: {s1['transfer_count']}\")\n",
        "    print(f\"skip_count: {s1.get('skip_count', 'N/A')}\")\n",
        "    print(f\"active_mask sum: {s1['active_mask'].sum().item() if s1['active_mask'] is not None else 'N/A'}\")\n",
        "    \n",
        "    print(f\"\\n--- threshold=0.9999 ---\")\n",
        "    print(f\"mask_count_before: {s2['mask_count_before']}\")\n",
        "    print(f\"transfer_count: {s2['transfer_count']}\")\n",
        "    print(f\"skip_count: {s2.get('skip_count', 'N/A')}\")\n",
        "    print(f\"active_mask sum: {s2['active_mask'].sum().item() if s2['active_mask'] is not None else 'N/A'}\")\n",
        "    \n",
        "    # 对比 x_after\n",
        "    if s1['x_after'] is not None and s2['x_after'] is not None:\n",
        "        diff_mask = (s1['x_after'] != s2['x_after'])\n",
        "        diff_pos = diff_mask.nonzero(as_tuple=True)[1].tolist()\n",
        "        print(f\"\\nx_after 不同的位置: {diff_pos}\")\n",
        "        for pos in diff_pos[:5]:  # 只显示前 5 个\n",
        "            t1 = s1['x_after'][0, pos].item()\n",
        "            t2 = s2['x_after'][0, pos].item()\n",
        "            print(f\"  位置 {pos}: t1={t1} ('{tokenizer.decode([t1])}'), t2={t2} ('{tokenizer.decode([t2])}')\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 分析 cos_sim 分布"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 查看 threshold=0.9999 时的 cos_sim 分布\n",
        "if div_block is not None and div_step is not None:\n",
        "    s2 = trace_t2[div_block][div_step]\n",
        "    \n",
        "    if s2['cos_sims'] is not None:\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Block {div_block}, Step {div_step} 的 cos_sim 分布 (threshold=0.9999)\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for j, cos_sims_j in enumerate(s2['cos_sims']):\n",
        "            if len(cos_sims_j) > 0:\n",
        "                avg = sum(cos_sims_j) / len(cos_sims_j)\n",
        "                min_cos = min(cos_sims_j)\n",
        "                max_cos = max(cos_sims_j)\n",
        "                \n",
        "                # 判定结果\n",
        "                skip = (min_cos >= SKIP_OUTLIER and avg > 0.9999)\n",
        "                status = \"SKIP\" if skip else \"KEEP\"\n",
        "                \n",
        "                # 只打印被 skip 的或者 cos_sim 接近阈值的\n",
        "                if skip or avg > 0.99:\n",
        "                    print(f\"Token {j:2d}: avg={avg:.6f}, min={min_cos:.6f}, max={max_cos:.6f} -> {status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 对比两个版本的 cos_sim（如果有）\n",
        "if div_block is not None and div_step is not None:\n",
        "    s1 = trace_t1[div_block][div_step]\n",
        "    s2 = trace_t2[div_block][div_step]\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"cos_sim 对比 (应该相同，因为输入相同)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if s1['cos_sims'] is not None and s2['cos_sims'] is not None:\n",
        "        for j in range(min(len(s1['cos_sims']), len(s2['cos_sims']))):\n",
        "            c1 = s1['cos_sims'][j]\n",
        "            c2 = s2['cos_sims'][j]\n",
        "            if len(c1) > 0 and len(c2) > 0:\n",
        "                avg1 = sum(c1) / len(c1)\n",
        "                avg2 = sum(c2) / len(c2)\n",
        "                if abs(avg1 - avg2) > 1e-6:\n",
        "                    print(f\"Token {j}: t1_avg={avg1:.6f}, t2_avg={avg2:.6f} <- 不同!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 深入检查：prev_hidden 形状问题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "threshold=0.9999: hidden_states 形状变化\n",
            "============================================================\n",
            "\n",
            "--- Block 0 ---\n",
            "  Step 1: prev=torch.Size([1, 147, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0 <- SHAPE MISMATCH!\n",
            "  Step 2: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 3: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 4: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 5: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 6: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 7: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "\n",
            "--- Block 1 ---\n",
            "  Step 1: prev=torch.Size([1, 147, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0 <- SHAPE MISMATCH!\n",
            "  Step 2: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 3: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 4: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 5: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 6: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "  Step 7: prev=torch.Size([1, 32, 4096]), curr=torch.Size([1, 32, 4096]), skip_count=0\n",
            "\n",
            "--- Block 2 ---\n",
            "\n",
            "--- Block 3 ---\n"
          ]
        }
      ],
      "source": [
        "# 检查每个 step 的 hidden_states 形状变化\n",
        "print(\"=\" * 60)\n",
        "print(\"threshold=0.9999: hidden_states 形状变化\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for nb in range(num_blocks):\n",
        "    print(f\"\\n--- Block {nb} ---\")\n",
        "    for step_info in trace_t2[nb]:\n",
        "        step = step_info['step']\n",
        "        prev_shape = step_info.get('prev_hidden_shape')\n",
        "        curr_shape = step_info.get('current_hidden_shape')\n",
        "        skip_count = step_info.get('skip_count', 0)\n",
        "        \n",
        "        if prev_shape and curr_shape:\n",
        "            # 只显示 Layer 0 的形状（代表性）\n",
        "            prev_l0 = prev_shape[0] if len(prev_shape) > 0 else None\n",
        "            curr_l0 = curr_shape[0] if len(curr_shape) > 0 else None\n",
        "            \n",
        "            shape_match = prev_l0 == curr_l0 if prev_l0 and curr_l0 else True\n",
        "            flag = \"\" if shape_match else \" <- SHAPE MISMATCH!\"\n",
        "            \n",
        "            print(f\"  Step {step}: prev={prev_l0}, curr={curr_l0}, skip_count={skip_count}{flag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 关键发现总结"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "关键发现总结\n",
            "============================================================\n",
            "\n",
            "1. 最终输出对比:\n",
            "   - threshold=1 输出长度: 128 tokens\n",
            "   - threshold=0.9999 输出长度: 128 tokens\n",
            "   - 不同位置数: 0\n",
            "\n",
            "2. 分歧点:\n",
            "   - Block: None\n",
            "   - Step: None\n",
            "   - 原因: no divergence found\n",
            "\n",
            "3. Token Skip 统计:\n",
            "   - threshold=1: 总共跳过 0 个 token（应该是 0）\n",
            "   - threshold=0.9999: 总共跳过 0 个 token\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"关键发现总结\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n1. 最终输出对比:\")\n",
        "print(f\"   - threshold=1 输出长度: {len(tokens_t1)} tokens\")\n",
        "print(f\"   - threshold=0.9999 输出长度: {len(tokens_t2)} tokens\")\n",
        "print(f\"   - 不同位置数: {len(diff_positions)}\")\n",
        "\n",
        "print(f\"\\n2. 分歧点:\")\n",
        "print(f\"   - Block: {div_block}\")\n",
        "print(f\"   - Step: {div_step}\")\n",
        "print(f\"   - 原因: {div_reason}\")\n",
        "\n",
        "# 统计 skip 次数\n",
        "total_skip_t1 = sum(\n",
        "    s.get('skip_count', 0) \n",
        "    for nb in range(num_blocks) \n",
        "    for s in trace_t1[nb]\n",
        ")\n",
        "total_skip_t2 = sum(\n",
        "    s.get('skip_count', 0) \n",
        "    for nb in range(num_blocks) \n",
        "    for s in trace_t2[nb]\n",
        ")\n",
        "\n",
        "print(f\"\\n3. Token Skip 统计:\")\n",
        "print(f\"   - threshold=1: 总共跳过 {total_skip_t1} 个 token（应该是 0）\")\n",
        "print(f\"   - threshold=0.9999: 总共跳过 {total_skip_t2} 个 token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 手动单步调试（可选）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug 初始化完成\n",
            "x_debug shape: torch.Size([1, 147])\n",
            "Block 0 范围: [19, 51)\n"
          ]
        }
      ],
      "source": [
        "# 如果需要更细致的调试，可以在这里手动运行单个 step\n",
        "# 先初始化\n",
        "\n",
        "x_debug = torch.full((1, prompt_len + GEN_LENGTH), MASK_ID, dtype=torch.long, device=device)\n",
        "x_debug[:, :prompt_len] = input_ids\n",
        "\n",
        "# 第一个 block 的范围\n",
        "s = prompt_len\n",
        "e = s + BLOCK_LENGTH\n",
        "\n",
        "print(f\"Debug 初始化完成\")\n",
        "print(f\"x_debug shape: {x_debug.shape}\")\n",
        "print(f\"Block 0 范围: [{s}, {e})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 完成\n",
            "转移了 1 个 token\n",
            "当前 block 剩余 mask: 31\n",
            "prev_hidden 层数: 33\n",
            "prev_hidden[0] shape: torch.Size([1, 147, 4096])\n"
          ]
        }
      ],
      "source": [
        "# Step 0: 完整前向（预热 KV cache）\n",
        "with torch.no_grad():\n",
        "    out0 = model(x_debug, use_cache=True, output_hidden_states=True)\n",
        "\n",
        "past_kv = out0.past_key_values\n",
        "prev_hidden_debug = out0.hidden_states\n",
        "\n",
        "# 初始采样\n",
        "global_mask = (x_debug == MASK_ID)\n",
        "global_mask[:, e:] = False\n",
        "x0_debug, transfer_debug = get_transfer_index(\n",
        "    out0.logits, 0., \"low_confidence\", global_mask, x_debug, None, THRESHOLD\n",
        ")\n",
        "x_debug = torch.where(transfer_debug, x0_debug, x_debug)\n",
        "\n",
        "print(f\"Step 0 完成\")\n",
        "print(f\"转移了 {transfer_debug.sum().item()} 个 token\")\n",
        "print(f\"当前 block 剩余 mask: {(x_debug[:, s:e] == MASK_ID).sum().item()}\")\n",
        "print(f\"prev_hidden 层数: {len(prev_hidden_debug)}\")\n",
        "print(f\"prev_hidden[0] shape: {prev_hidden_debug[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1 对比:\n",
            "  threshold=1 logits shape: torch.Size([1, 32, 126464])\n",
            "  threshold=0.9999 logits shape: torch.Size([1, 32, 126464])\n",
            "  logits max diff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Step 1: 对比 threshold=1 和 threshold=0.9999\n",
        "replace_pos = torch.zeros_like(x_debug, dtype=torch.bool)\n",
        "replace_pos[:, s:e] = True\n",
        "\n",
        "# threshold=1\n",
        "with torch.no_grad():\n",
        "    out1_t1 = model(\n",
        "        x_debug[:, s:e],\n",
        "        past_key_values=past_kv,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_pos,\n",
        "        output_hidden_states=True,\n",
        "        skip_layer_k=SKIP_LAYER_K,\n",
        "        skip_threshold=1.0,\n",
        "        skip_outlier=SKIP_OUTLIER,\n",
        "        prev_hidden=prev_hidden_debug,\n",
        "    )\n",
        "\n",
        "# threshold=0.9999\n",
        "with torch.no_grad():\n",
        "    out1_t2 = model(\n",
        "        x_debug[:, s:e],\n",
        "        past_key_values=past_kv,\n",
        "        use_cache=True,\n",
        "        replace_position=replace_pos,\n",
        "        output_hidden_states=True,\n",
        "        skip_layer_k=SKIP_LAYER_K,\n",
        "        skip_threshold=0.9999,\n",
        "        skip_outlier=SKIP_OUTLIER,\n",
        "        prev_hidden=prev_hidden_debug,\n",
        "    )\n",
        "\n",
        "print(f\"Step 1 对比:\")\n",
        "print(f\"  threshold=1 logits shape: {out1_t1.logits.shape}\")\n",
        "print(f\"  threshold=0.9999 logits shape: {out1_t2.logits.shape}\")\n",
        "\n",
        "logits_diff = (out1_t1.logits - out1_t2.logits).abs().max().item()\n",
        "print(f\"  logits max diff: {logits_diff}\")\n",
        "\n",
        "if logits_diff > 0:\n",
        "    print(f\"\\n  >>> logits 不同！Token Skip 产生了影响 <<<\")\n",
        "    \n",
        "    # 查看 hidden_states 形状\n",
        "    print(f\"\\n  hidden_states 形状对比:\")\n",
        "    for i, (h1, h2) in enumerate(zip(out1_t1.hidden_states, out1_t2.hidden_states)):\n",
        "        if h1.shape != h2.shape:\n",
        "            print(f\"    Layer {i}: t1={h1.shape}, t2={h2.shape} <- 不同!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
